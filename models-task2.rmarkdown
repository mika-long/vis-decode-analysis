---
title: "Models Task 2"
format:
  html: 
    code-fold: false
    reference-location: margin
    df-print: paged
    toc: true
    code-overflow: wrap
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
library(sgt)
library(numDeriv)
library(ggdist)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
```



# Set up 

read in data and write functions for processing: 



```{r}
#| label: read-in-data
#| code-fold: true

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)
```

```{r}
#| label: define-function
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| label: define-special-dfs
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```




# Task 2 -- Find highest point on curve 

First we get the data: 



```{r}
#| label: get task 2 data
#| code-fold: true 

task2_df <- task_df %>% filter(task == "task2") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.ans.x = param.mu, 
         data.ans.y = dsgt(param.mu, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE),
         data.select.grad = numDeriv::grad(dsgt, data.select.x, mu = param.mu, sigma = param.sigma, lambda = param.lambda, p = param.p, q = param.q, mean.cent = FALSE), 
         data.select.angle = atan(data.select.grad) * 180 / pi) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  # calculate things related to participants' selection 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM),
         va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>% 
  # calculate things related to answer 
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x), 
         pixel.ans.y = data_to_pixel_y(data.ans.y), 
         phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM), 
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM),
         va.ans.x = vis_angle(phy.ans.x, dist_to_screen),
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen))
```



Some preliminary checks of relations, focusing on the **y** axes: 



```{r}
#| layout-ncol: 2

task2_df %>% ggplot(aes(y = phy.ans.y - phy.select.y, x = dist_to_screen)) + 
  geom_point()

task2_df %>% ggplot(aes(y = va.ans.y - va.select.y, x = dist_to_screen)) + 
  geom_point()

task2_df %>% ggplot(aes(x = va.ans.y - va.select.y)) + geom_dots()
task2_df %>% ggplot(aes(x = phy.ans.y - phy.select.y)) + geom_dots()

task2_df %>% mutate(error_va = va.ans.y - va.select.y) %>% 
  mutate(error_va = ifelse(abs(error_va) < 1e-8, 0, error_va)) %>%  
  # filter(error_va != 0) %>% 
  ggplot(aes(x = va.ans.x - va.select.x)) + 
  geom_dots()
```



Now let's look at things focusing on the **x** axes: 



```{r}
#| layout-ncol: 2


task2_df %>% ggplot(aes(x = data.ans.x - data.select.x)) + 
  geom_dots()

task2_df %>% ggplot(aes(x = va.ans.x - va.select.x)) + 
  geom_dots() + 
  facet_wrap(~participantId)

task2_df %>% ggplot(aes(y = va.ans.x - va.select.x, x = dist_to_screen)) + 
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = va.select.x - va.ans.x, x = param.lambda)) + 
  geom_point(alpha = 0.5)
```



## Model 1 -- Vis Angle space

Let us first try to fit `va.ans.x - va.select.x` with two different models --- normal and laplace. 

### Model 1.1 --- Laplace distribution 

$$
\begin{align}
\text{error_va}_{i} &\sim \text{Laplace}(\mu_{\text{PID}[i]}, b_{\text{PID}[i]}) \\
\mu_j &\sim \mathcal{N}(0, 0.25), j \in [13] \\ 
b_j &\sim \mathcal{N}(0, 1), j \in [13] 
\end{align}
$$

Specify formula. There is no native implementation of the Laplace distribution^[except for the [`asym_laplace` function](https://paulbuerkner.com/brms/reference/AsymLaplace.html), which doesn't seem to help. Maybe we'll need to implement a custom function using `stan` ... ]:

:::{.callout-note}
There is a [wiki](https://en.wikipedia.org/wiki/Laplace_distribution) about Laplace distribution, which defines the pdf of a Laplace distribution to be $f(x | \mu, b) = \frac{1}{2b} \exp(-\frac{|x - \mu|}{b})$, where $\mu$ is a location parameter, and $b > 0$ is a scale parameter. And this is what the [stan manual](https://en.wikipedia.org/wiki/Laplace_distribution) defines the pdf of the **double exponential** distribution  --- $f(y|\mu, \sigma) = \frac{1}{2\sigma} \exp(- \frac{|y - \mu|}{\sigma})$, basically everything is the same except they're using different notations for the scale parameter ...  
:::



```{r}
# log of the laplace pdf 
laplace_lpdf <- "
real laplace_lpdf(real y, real mu, real sigma) {
  return -log(2 * sigma) - abs(y - mu) / sigma;
}
"

# define custom family 
c_family = custom_family(
  "laplace", 
  dpars = c("mu", "sigma"), 
  links = c("identity", "log"), 
  lb = c(NA, 0), 
  type = "real"
)

f <- bf(error_va ~ 1 + (1 | participantId),
        family = c_family)

# get prior
task2_df %>% mutate(error_va = va.select.x - va.ans.x) %>%
get_prior(formula = f,
          data = .)

# define prior 
p <- c(
  prior(normal(0, 0.5), class = "Intercept"), 
  prior(normal(0, 0.25), class = "sd", group = "participantId"), 
  prior(normal(0, 1), class = "sigma")
)
```



What if we use the `make_stancode` function to see what the corresponding stan code looks like?



```{r}
task2_df %>% mutate(error_va = va.ans.x - va.select.x) %>% 
  make_stancode(f, data = ., family = c_family, stanvars = stanvar(scode = laplace_lpdf, block = "functions"))
```




Fit the model:



```{r}
#| output: false 

m1.1 <- task2_df %>% mutate(error_va = va.ans.x - va.select.x) %>%
  brm(
    formula = f, 
    family = c_family, 
    stanvars = stanvar(scode = laplace_lpdf, block = "functions"), 
    prior = p, 
    data = ., 
    chains = 4, 
    # cores = 4, 
    # control = list(adapt_delta = 0.95), 
    file = "models/task2-m1.1"
  )
```



Check the fitted model:



```{r}
summary(m1.1)
plot(m1.1)
```



We also need to define our own custom function for posterior predictive checks --- NOTE that the following does not run ... 



```{r}
# Define a random number generator for Laplace distribution
laplace_rng <- function(mu, sigma) {
  # Generate from Laplace distribution using uniform transformation
  u <- runif(length(mu)) - 0.5
  mu + sigma * sign(u) * (-log(1 - 2 * abs(u)))
}

# Define the posterior predict function for our custom family
posterior_predict_laplace <- function(i, prep, ...) {
  mu <- brms::get_dpar(prep, "mu", i = i)
  sigma <- brms::get_dpar(prep, "sigma", i = i)
  laplace_rng(mu, sigma)
}

# Make this function available globally
assign("posterior_predict_laplace", posterior_predict_laplace, envir = .GlobalEnv)
```

```{r}
pp_check(m1.1, ndraws = 100)
```



Compare to the later fit, this fit is much better ... 


### Model 2.2 --- Normal 

Math model: 

$$
\begin{align}
\text{error_va}_{i} &\sim \text{Normal}(\mu_{\text{PID}[i]}, \sigma_{\text{PID}[i]}) \\
\mu_j &\sim \mathcal{N}(0, 0.25), j \in [13] \\ 
\sigma_j &\sim \mathcal{N}(0, 1), j \in [13] 
\end{align}
$$

The corresponding `brms` formula would be as follows:



```{r}
#| label: task 2 formula 2

f2 <- bf(error_va ~ 1 + (1 | participantId),
        family = gaussian())

# get prior
task2_df %>% mutate(error_va = va.select.x - va.ans.x) %>%
get_prior(formula = f2,
          data = .)

p <- c(
  prior(normal(0, 0.5), class = "Intercept"), 
  prior(normal(0, 0.25), class = "sd", group = "participantId"), 
  prior(normal(0, 1), class = "sigma")
)
```



Let's fit the model:



```{r}
#| output: false
#| label: task2-fit-model-3

m1.2 <- task2_df %>% mutate(error_va = va.ans.x - va.select.x) %>%
  brm(
    formula = f2,
    data = .,
    prior = p,
    control = list(adapt_delta = 0.95), # note this is higher than 0.95
    chains = 4,
    iter = 8000, # more iterations
    warmup = 2000, # higher than usual
    file="models/task2-m1.2",
    save_pars = save_pars(all=TRUE)
  )
```



Let's see some fitted results:



```{r}
summary(m1.2)
plot(m1.2)
# posterior predictive check
pp_check(m1.2, ndraws = 100)
```



The fit doesn't look that good lol. 

### LOO comparisons 

We can also perform loo comparison between `m1.1` and `m1.2`: 



```{r}
expose_functions(m1.1, vectorize = TRUE)

# Define log-likelihood function for Laplace distribution
laplace_lpdf <- function(y, mu, sigma) {
  -log(2 * sigma) - abs(y - mu) / sigma
}

# Define log-likelihood function for brms
log_lik_laplace <- function(i, prep) {
  mu <- brms::get_dpar(prep, "mu", i = i)
  sigma <- brms::get_dpar(prep, "sigma", i = i)
  y <- prep$data$Y[i]
  laplace_lpdf(y, mu, sigma)
}

# Make this function available globally
assign("log_lik_laplace", log_lik_laplace, envir = .GlobalEnv)
```



Alright let's do the actual comparison: 



```{r}
loo(m1.1, m1.2)
```



From the `pp_check`s and the loo comparison it is clear that the Laplace distribution (aka Double Exponential) distribution fits the model better. 


<!-- Hmm the half-normal model doesn't look quite right, perhaps this follows closer to an exponential distribution? Also we need to take care of teh number of times thigns were zero, so this should be a **zero-inflated exponential model**. Something like the following:  -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- \text{error}_{\text{va}}[i] &\sim \text{Exponential}(\lambda_{\text{PID}[i]}) \\  -->
<!-- \lambda &= \exp(\alpha + u_j) \\  -->
<!-- \alpha &\sim \mathcal{N}(0, 1) \\  -->
<!-- u_j &\sim \mathcal{N}(0, \sigma_u^2) \\  -->
<!-- \sigma_u &\sim \text{Half-Cauchy}(0, 5)  -->
<!-- \end{align} -->
<!-- $$ -->







<!-- :::{.caution} -->
<!-- While doing the fitting I found some issues ... For task 2, `error_va = va.ans.y - va.select.y` should always be non-negative, but I got some negative values that are very, very small. I am setting a tolerance of `1e-12` --- any value smaller than this range will be regarded as 0. See the following for the negative values  -->

<!-- ```{r} -->
<!-- task2_df %>% mutate(error_va = va.ans.y - va.select.y) %>% filter(error_va < 0) %>%  -->
<!--   select(data.select.y, data.ans.y) %>%  -->
<!--   mutate(diff = data.select.y - data.ans.y) %>%  -->
<!--   pull(diff) -->
<!-- ``` -->

<!-- ::: -->

<!-- Fit the model:  -->

<!-- ```{r} -->
<!-- #| output: false -->
<!-- #| label: task2-fit-model -->

<!-- m2.1 <- task2_df %>% mutate(error_va = va.ans.y - va.select.y) %>%  -->
<!--   mutate(error_va = ifelse(abs(error_va) < 1e-8, 0, error_va)) %>%  -->
<!--   brm(formula = f, -->
<!--       data = .,  -->
<!--       prior = p, -->
<!--       control = list(adapt_delta = 0.99), # note this is higher than 0.95 -->
<!--       chains = 4,  -->
<!--       iter = 8000, # more iterations -->
<!--       warmup = 2000, # higher than usual  -->
<!--       file="models/m2.1",  -->
<!--       save_pars = save_pars(all=TRUE) -->
<!--       ) -->
<!-- ``` -->



<!-- ## Model 2 -- Physical -->

<!-- Math model:  -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{error}_{\text{phy}}[i] &\sim \text{Half-Normal}(\sigma[i]) \\  -->
<!-- \sigma[i] &= \texttt{dist_to_screen} \times \hat{\sigma}[i]\\  -->
<!-- \hat{\sigma}[i] &\sim \text{Log-Normal}(0, 1) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- Formula:  -->

<!-- ```{r} -->
<!-- f <- bf(error_phy | trunc(lb = 0) ~ 0,  -->
<!--         sigma ~ 0 + offset(log(dist_to_screen)) + (1 | participantId),  -->
<!--         family = gaussian()) -->

<!-- p <- prior(lognormal(0, 1), class = "sd", dpar="sigma", group="participantId") -->
<!-- ``` -->

<!-- Fit model:  -->

<!-- ```{r} -->
<!-- #| output: false  -->

<!-- m2.2 <- task2_df %>%  -->
<!--   mutate(error_phy = phy.ans.y - phy.select.y) %>%  -->
<!--   # note that the usual tolerance of 1e-12 is not good enough  -->
<!--   # mutate(error_phy = ifelse(abs(error_phy) < 1e-8, 0, error_phy)) %>%  -->
<!--   # let's just make everything negative to be 0  -->
<!--   mutate(error_phy = ifelse(error_phy < 0, 0, error_phy)) %>%  -->
<!--   brm(formula = f, -->
<!--       data = .,  -->
<!--       prior = p, -->
<!--       control = list(adapt_delta = 0.99), # note this is higher than 0.95 -->
<!--       chains = 4,  -->
<!--       iter = 8000, # more iterations -->
<!--       warmup = 2000, # higher than usual  -->
<!--       file="models/m2.2",  -->
<!--       save_pars = save_pars(all=TRUE) -->
<!--       ) -->
<!-- ``` -->

<!-- Check fit:  -->

<!-- ```{r} -->
<!-- summary(m2.2) -->
<!-- plot(m2.2) -->
<!-- pp_check(m2.2, ndraws = 100) -->
<!-- ``` -->

<!-- ## Compare model 1 and model 2  -->

<!-- ```{r} -->
<!-- m2.1 <- add_criterion(m2.1, "loo", moment_match=TRUE, recompile=TRUE) -->
<!-- m2.2 <- add_criterion(m2.2, "loo", moment_match = TRUE, recompile=TRUE) -->

<!-- loo_compare(m2.1, m2.2) -->
<!-- ``` -->

<!-- The visual angle version is also better.  -->

<!-- --- -->

<!-- Then we check to make sure that things are correct:  -->

<!-- ```{r} -->
<!-- #| output: false  -->
<!-- #| code-fold: true  -->
<!-- #| label: task 2 verify  -->

<!-- task2_df %>% ggplot(aes(x = data.select.x - data.ans.x)) +  -->
<!--   geom_dots() -->

<!-- task2_df %>% ggplot(aes(x = data.select.y - data.ans.y)) +  -->
<!--   geom_dots() -->

<!-- task2_df %>% ggplot(aes(x = pixel.select.x - pixel.ans.x)) +  -->
<!--   geom_dots() -->

<!-- task2_df %>% ggplot(aes(x = pixel.select.y - pixel.ans.y)) +  -->
<!--   geom_dots() -->

<!-- task2_df %>% ggplot(aes(x = phy.select.x - phy.ans.x)) +  -->
<!--   geom_dots()   -->

<!-- task2_df %>% ggplot(aes(x = phy.select.y - phy.ans.y)) +  -->
<!--   geom_dots()  -->

<!-- task2_df %>% ggplot(aes(x = va.select.x - va.ans.x)) +  -->
<!--   geom_dots()   -->

<!-- task2_df %>% ggplot(aes(x = va.select.y - va.ans.y)) +  -->
<!--   geom_dots()   -->
<!-- ``` -->

<!-- Note that `grad.x` is calcultaing the angle of the slope^[i.e. gradient] at selection `x`. It is NOT the perceived angle.  -->

<!-- Let's plot the relation between the angle selected and the parameters:  -->

<!-- ```{r} -->
<!-- #| code-fold: true  -->
<!-- #| layout-ncol: 2 -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = param.p)) + -->
<!--   geom_point(alpha = 0.5) -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = param.q)) + -->
<!--   geom_point(alpha = 0.5) -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = param.lambda)) + -->
<!--   geom_point(alpha = 0.5) -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = param.mu)) + -->
<!--   geom_point(alpha = 0.5) -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = param.sigma)) + -->
<!--   geom_point(alpha = 0.5) -->

<!-- task2_df %>% ggplot(aes(y = data.select.grad, x = dist_to_screen)) + -->
<!--   geom_point(alpha = 0.5) -->
<!-- ``` -->

<!-- The only thing that is worth noting is the relationship between `data.select.grad` and `dist_to_screen`. What about the corresponding angle?  -->

<!-- ```{r} -->
<!-- task2_df %>% ggplot(aes(y = data.select.angle, x = dist_to_screen)) + -->
<!--   geom_point(alpha = 0.5) -->
<!-- ``` -->

<!-- --- -->

<!-- How to calculate the actual angle, based on the selection? First we can check out the distribution of calculated angle (relative to the highest point?) -->

<!-- ```{r} -->
<!-- task2_df %>%  -->
<!--   mutate(diff.x = phy.select.x - phy.ans.x,  -->
<!--          diff.y = phy.select.y - phy.ans.y,  -->
<!--          angle = atan(diff.y / diff.x) * 180 / pi) %>%  -->
<!--   ggplot(aes(x = angle)) +  -->
<!--   geom_dots() -->
<!-- ``` -->

<!-- We are seeing these extreme outliers because `diff.x` is very, very small, almost close to 0. The above calculations does not deal well with very, very small denominators. Doing things in the visual angle space gives the same outliers. Let's just round things up to 3:  -->

<!-- ```{r} -->
<!-- task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x,  -->
<!--                     diff.y = phy.select.y - phy.ans.y) %>%  -->
<!--   mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>%  -->
<!--   mutate(angle = case_when(diff.x == 0 ~ 0,  -->
<!--                            diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>%  -->
<!--   ggplot(aes(x = angle)) +  -->
<!--   geom_dots() -->

<!-- task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x,  -->
<!--                     diff.y = phy.select.y - phy.ans.y) %>%  -->
<!--   mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>%  -->
<!--   mutate(angle = case_when(diff.x == 0 ~ 0,  -->
<!--                            diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>%  -->
<!--   ggplot(aes(x = angle, y = data.select.angle)) +  -->
<!--   geom_point(alpha = 0.5) +  -->
<!--   geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed") -->
<!-- ``` -->

<!-- This just means that if we were to use the highest point and the selected point as ways to calculate the actual angle, then it is **larger** than the actual slope at the selected point.  -->

<!-- Is this newly calculated `angle` going to be related to `dist_to_screen`?  -->

<!-- ```{r} -->
<!-- task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x,  -->
<!--                     diff.y = phy.select.y - phy.ans.y) %>%  -->
<!--   mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>%  -->
<!--   mutate(angle = case_when(diff.x == 0 ~ 0,  -->
<!--                            diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>%  -->
<!--   ggplot(aes(x = dist_to_screen, y = angle)) +  -->
<!--   geom_point(alpha = 0.5) -->
<!-- ``` -->

<!-- Compare to the graph before, the difference is that this one has a bigger variance.  -->

<!-- What if we use the numbers in the visual angle space and not the physical space?  -->

<!-- ```{r} -->
<!-- task2_df %>% mutate(diff.x = va.select.x - va.ans.x,  -->
<!--                     diff.y = va.select.y - va.ans.y) %>%  -->
<!--   mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>%  -->
<!--   mutate(angle = case_when(diff.x == 0 ~ 0,  -->
<!--                            diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>%  -->
<!--   ggplot(aes(x = dist_to_screen, y = angle)) +  -->
<!--   geom_point(alpha = 0.5) -->
<!-- ``` -->

<!-- Hmm doesn't seem to be much different, which is to be expected, as slope is unit-less.  -->

<!-- ---  -->

