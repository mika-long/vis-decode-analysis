---
title: "Preliminary Model building"
format:
  html: 
    code-fold: false
    reference-location: margin
    df-print: paged
    toc: true
    code-overflow: wrap
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
library(sgt)
library(numDeriv)
library(ggdist)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
library(bayesplot)
```

:::{.callout-caution}
Per Matt's advice: **Think in a generative way**
:::

# Read in data and all processing 

- All the things selected by the participants will be named `xxx.select.xxx` 
- All the things related to the actual answer will be named `xxx.ans.xxx`

```{r}
#| label: read-in-data
#| code-fold: true

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)
```

```{r}
#| label: define-function
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| label: define-special-dfs
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```

# Task 5 -- Project from dot to axes

Get data: 

```{r}
#| label: task 5 load data
#| code-fold: true 

task5_df <- task_df %>% filter(task == "task5") %>% 
  select(participantId, task, id, data.select.x, data.select.y, slider.x, slider.y) %>% 
  rename(data.ans.x = data.select.x, 
         data.ans.y = data.select.y, 
         data.select.x = slider.x, 
         data.select.y = slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  # do all the calculations for user's selected 
  mutate(pixel.select.x = data_to_pixel_x(data.select.x), 
         pixel.select.y = data_to_pixel_y(data.select.y), 
         phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM),
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM), 
         va.select.x = vis_angle(phy.select.x, dist_to_screen),
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>% 
  # do all the calculations for the actual answer
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x),
         pixel.ans.y = data_to_pixel_y(data.ans.y),
         phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM),
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM), 
         va.ans.x = vis_angle(phy.ans.x, dist_to_screen),
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen))
```

Verify that the results are in the right scale: 

```{r}
#| output: false 
#| code-fold: true
#| label: task 5 verify 

task5_df %>% ggplot(aes(x = pixel.select.x - pixel.ans.x)) + 
  geom_dots()

task5_df %>% ggplot(aes(x = pixel.select.y - pixel.ans.y)) + 
  geom_dots()

task5_df %>% ggplot(aes(x = phy.select.x - phy.ans.x)) + 
  geom_dots()  

task5_df %>% ggplot(aes(x = phy.select.y - phy.ans.y)) + 
  geom_dots() 

task5_df %>% ggplot(aes(x = va.select.x - va.ans.x)) + 
  geom_dots()  

task5_df %>% ggplot(aes(x = va.select.y - va.ans.y)) + 
  geom_dots()  

```

## Project from dot to X axes

Distribution of signed error: 

```{r}
#| code-fold: true

task5_df %>% ggplot(aes(x = va.select.x - va.ans.x)) + 
  geom_dots() +
  geom_vline(xintercept = 0, linetype="dashed", color="black") + 
  labs(title="Task 5 - Project from Dot to X-Axes - Vis Angle")
```

Relationship between signed error (in visual angle space) and distance to X-axes: 

```{r}
#| code-fold: true

task5_df %>% ggplot(aes(y = va.ans.y, x = va.select.x - va.ans.x)) + 
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 0, linetype="dashed", color="gray") + 
  labs(title="Task 5 - Project from Dot to X-Axes - Vis Angle")
```

### Model 1 -- Vis Angle 

Model 1: 

$$
\begin{aligned}
\text{error}_{\text{va}}[i] &\sim \mathcal{N}(0, \sigma[i]) \\ 
\sigma[i] &= a \times \texttt{va.ans.y} \times \hat{\sigma}[i] \\
\equiv \log(\sigma[i]) &= \log(a) + \log(\texttt{va.ans.y}) + \log(\hat{\sigma}[i])\\ 
\log(\hat{\sigma}[i]) &\sim \mathcal{N}(0, 1)
\end{aligned}
$$

:::{.caution-note}
We could also model $\sigma[i] = \texttt{va.ans.y} \times \hat{\sigma}[i] \equiv \log(\sigma[i]) = \log(\texttt{va.ans.y}) + \log(\hat{\sigma}[i])$ that would just imply we change the formula from `sigma ~ 1 + offset(log(va.ans.y)) + (1 | participantId) ` to `sigma ~ 0 + offset(log(va.ans.y)) + (1 | participantId)`.
:::

Let's first get a sense of what the parameters and priors look like: 

```{r} 
#| label: task 5 x formula

# specify formula 
f <- bf(error_va ~ 0, 
        sigma ~ 0 + offset(log(va.ans.y)) + (1 | participantId))

# get prior 
task5_df %>% mutate(error_va = va.select.x - va.ans.x) %>%
get_prior(formula = f, 
          data = .)
```

To match this to our math specificaiton, we need to set the prior to be `prior(constant(1), class = "sd", group = "participantId")`. 

```{r}
#| label: task5-fit-model-1-x 
#| output: false

# specify prior 
p <- prior(constant(1), class = "sd", group = "participantId", dpar="sigma")

m5.1.x <- task5_df %>% mutate(error_va = va.select.x - va.ans.x) %>%
  # account for numerical precision 
  mutate(error_va = ifelse(abs(error_va) < tolerance, 0, error_va)) %>% 
  filter(va.ans.y != 0) %>% # account for weird bugs 
  brm(
    formula = f,
    data = .,
    family = gaussian(),
    prior = p,
    control = list(adapt_delta = 0.95),
    chains = 4, 
    iter = 6000, 
    file="models/m5.1.x", 
    save_pars = save_pars(all = TRUE)
)
```

What does the fit look like? 

```{r}
summary(m5.1.x)
# mixing check 
plot(m5.1.x)
# posterior predictive check 
pp_check(m5.1.x, ndraws = 100)
```

Other kinds of plotting ... 

```{r}
get_variables(m5.1.x)
```

Given that we have specified out prior to have variance 1 (recall that $\hat{\sigma}[i] \sim \text{Log-Normal}(0, 1)$), if we were to plot it it will just be a dot on 1: 

```{r}
#| column: margin

m5.1.x %>% mcmc_areas(pars = vars(starts_with("sd_")))
```

What is more interesting is the $\hat{\sigma}[i]$ for each participant, which we can visualize as follows: 

```{r}
m5.1.x %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  # stat_pointinterval() 
  stat_halfeye()
```

:::{.column-margin}
The outlier here (`5f37...`) is Maryam. 
:::

### Model 2 -- Physical

There could also be two versions of this ... one is additive and the other is duplicative ... 

$$
\begin{aligned}
\text{error}_{\text{phy}}[i] &\sim \mathcal{N}(0, \sigma[i]) \\ 
\sigma[i] &= \text{dist}[i] \times \texttt{phy.ans.y}[i] \times \hat{\sigma}[i] \\ 
\text{or} \;\;\; \sigma[i] &= (\text{dist}[i] + \texttt{phy.ans.y[i]}) \times \hat{\sigma}[i]\\ 
\hat{\sigma}[i] &\sim \mathcal{N}(0, 1)
\end{aligned}
$$

Write the formula and get a sense of prior: 

```{r}
#| label: task5-x-model2-formula

# formula
f1 <- bf(error_phy ~ 0, 
        sigma ~ 0 + offset(log(phy.ans.y)) + offset(log(dist_to_screen)) + (1 | participantId))

f2 <- bf(error_phy ~ 0, 
        sigma ~ 0 + offset(log(phy.ans.y + dist_to_screen))  + (1 | participantId))

# get prior 
task5_df %>% mutate(error_phy = phy.select.x - phy.ans.x) %>%
  get_prior(formula = f1, 
            data = .)
```

```{r}
#| label: task5-fit-model-2-x
#| output: false 

p <- prior(constant(1), class = "sd", group = "participantId", dpar="sigma")

m5.2.1.x <- task5_df %>% 
  mutate(error_phy = phy.select.x - phy.ans.x) %>% 
  mutate(error_phy = ifelse(abs(error_phy) < tolerance, 0, error_phy)) %>% 
  filter(va.ans.y != 0) %>% # account for weird bugs
  brm(formula = f1,
      data = .,
      family = gaussian(),
      prior = p,
      control = list(adapt_delta = 0.95),
      chains = 4,
      iter = 6000, 
      warmup=1000,
      file="models/m5.2.1.x", 
      save_pars = save_pars(all = TRUE)
  )

m5.2.2.x <- task5_df %>% 
  mutate(error_phy = phy.select.x - phy.ans.x) %>% 
  mutate(error_phy = ifelse(abs(error_phy) < tolerance, 0, error_phy)) %>% 
  filter(va.ans.y != 0) %>% # account for weird bugs
  brm(formula = f2,
      data = .,
      family = gaussian(),
      prior = p,
      control = list(adapt_delta = 0.95),
      chains = 4,
      iter = 6000, 
      warmup=1000,
      file="models/m5.2.2.x",
      save_pars = save_pars(all = TRUE)
  )
```

Let's check the fit: 

```{r}
summary(m5.2.1.x)
summary(m5.2.2.x)
plot(m5.2.1.x)
plot(m5.2.2.x)
# posterior check 
pp_check(m5.2.1.x, ndraws = 100)
pp_check(m5.2.2.x, ndraws = 100)
```

Let's visualize things in this sphere: 

```{r}
#| layout-ncol: 2

m5.2.1.x %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  stat_halfeye()

m5.2.2.x %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  # stat_pointinterval() 
  stat_halfeye()
```

This looks very similar to the trend we had before. 
<!-- One thing however that I'm not getting --- when I'm using `stat_pointinterval` the y-axis range is cropped to 0.006, but when it's using `stat_halfeye` it crops to 0.02 ... why is this the case??  -->

### Comparing model 1 and 2

Let's compare the two: 

```{r}
# first we add the loo criterion to existing models 
m5.1.x <- add_criterion(m5.1.x, "loo", moment_match=TRUE, recompile=TRUE)
m5.2.1.x <- add_criterion(m5.2.1.x, "loo", moment_match = TRUE, recompile=TRUE)
m5.2.2.x <- add_criterion(m5.2.2.x, "loo")

loo_compare(m5.2.1.x, m5.2.2.x)
loo_compare(m5.1.x, m5.2.1.x, m5.2.2.x)
```

Based on the result of the LOO comparison, the visual angle model `m5.1.x` is preferred^[though it is unclear whether using `loo` between models in different space still makes sense ... ]. Based on the model comparison results for those in the physical space, additive `dist_to_screen + dist_to_axes` is better than multiplicative. 

## Project from dot to Y Axes

Distribution of signed error: 

```{r}
#| code-fold: true 

task5_df %>% ggplot(aes(x = va.select.y - va.ans.y)) + 
  geom_dots() + 
  geom_vline(xintercept = 0, color="black", linetype="dashed") + 
  labs(title = "Task 5 - Project from dot to Y-Axes - Visual Angle")
```

Relationship between signed error and distance to x-axes (in visual angle space): 

```{r}
#| code-fold: true

task5_df %>% ggplot(aes(x = va.ans.x, y = va.select.y - va.ans.y)) + 
  geom_point(alpha = 0.5) + 
  geom_hline(yintercept = 0, linetype="dashed", color="gray") + 
  labs(title = "Task 5 - Project from dot to Y-Axes - Visual Angle")
```

We apply the same approach, fit two models, and compare them. 

### Model 1 -- Vis Angle

```{r}
f <- bf(error_va ~ 0, 
        sigma ~  0 + offset(log(va.ans.x)) + (1 | participantId))
p <- prior(constant(1), class = "sd", group = "participantId", dpar="sigma")
```

Fitting the model: 

```{r}
#| label: task5-fit-model-1-y 
#| output: false 

m5.1.y <- task5_df %>% 
  mutate(error_va = va.select.y - va.ans.y) %>%
  mutate(error_va = ifelse(abs(error_va) < tolerance, 0, error_va)) %>%
  filter(va.ans.y != 0) %>% # account for weird bugs
  brm(
    formula = f,
    data = .,
    family = gaussian(),
    prior = p,
    control = list(adapt_delta = 0.95),
    chains = 4, 
    iter = 6000, 
    file="models/m5.1.y", 
    save_pars = save_pars(all=TRUE)
)
```

Check the output: 

```{r}
summary(m5.1.y)
plot(m5.1.y)
pp_check(m5.1.y, ndraws = 100)
```

How are the $\hat{\sigma}[i]$ distributed? 

```{r}
m5.1.y %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  # stat_pointinterval() 
  stat_halfeye()
```

### Second model 

```{r}
f <- bf(error_phy ~ 0, 
        sigma ~ 0 + offset(log(phy.ans.x + dist_to_screen))+ (1 | participantId))
p <- prior(constant(1), class = "sd", group = "participantId", dpar="sigma")
```

Fitting the model: 

```{r}
#| output: false 
#| label: task5-y-model-2-fit

m5.2.y <- task5_df %>% 
  mutate(error_phy = phy.select.y - phy.ans.y) %>%
  mutate(error_phy = ifelse(abs(error_phy) < tolerance, 0, error_phy)) %>% 
  filter(va.ans.y != 0) %>% # account for weird bugs
  brm(
    formula = f,
    data = .,
    family = gaussian(),
    prior = p,
    control = list(adapt_delta = 0.95),
    chains = 4, 
    iter = 6000, 
    file="models/m5.2.y",
    save_pars = save_pars(all=TRUE)
)
```

Check the result: 

```{r}
summary(m5.2.y)
plot(m5.2.y)
pp_check(m5.2.y, ndraws = 100)
```

How are the $\hat{\sigma}[i]$ distributed? 

```{r}
m5.2.y %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  # stat_pointinterval() 
  stat_halfeye()
```

### Compare the two models 

```{r}
m5.1.y <- add_criterion(m5.1.y, "loo", moment_match=TRUE, recompile=TRUE)
m5.2.y <- add_criterion(m5.2.y, "loo", moment_match = TRUE, recompile=TRUE)

loo_compare(m5.1.y, m5.2.y)
```

Similarly, we see that the visual angle model is much better. 

# Task 3 -- Find point on curve where `y == 0.5`

First we get the data:

```{r}
task3_df <- task_df %>% filter(task == "task3") %>% 
  select(-slider.x, -slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  # do all the calculations for user's selected 
  mutate(pixel.select.x = data_to_pixel_x(data.select.x), 
         pixel.select.y = data_to_pixel_y(data.select.y), 
         phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM),
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM), 
         va.select.x = vis_angle(phy.select.x, dist_to_screen),
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>% 
  mutate(data.ans.y = 0.5, 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  # do all the calculations for the actual answer
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x),
         pixel.ans.y = data_to_pixel_y(data.ans.y),
         phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM),
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM), 
         va.ans.x = vis_angle(phy.ans.x, dist_to_screen),
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen))
```

Verify that the scales are correct: 

```{r}
#| output: false 
#| code-fold: true
#| label: task 3 verify 

# verify that the results are of correct scale
task3_df %>% ggplot(aes(x = data.select.x - data.ans.x)) + 
  geom_dots()

task3_df %>% ggplot(aes(x = data.select.y - data.ans.y)) + 
  geom_dots()

task3_df %>% ggplot(aes(x = pixel.select.x - pixel.ans.x)) + 
  geom_dots()

task3_df %>% ggplot(aes(x = pixel.select.y - pixel.ans.y)) + 
  geom_dots()

task3_df %>% ggplot(aes(x = phy.select.x - phy.ans.x)) + 
  geom_dots()  

task3_df %>% ggplot(aes(x = phy.select.y - phy.ans.y)) + 
  geom_dots() 

task3_df %>% ggplot(aes(x = va.select.x - va.ans.x)) + 
  geom_dots()  

task3_df %>% ggplot(aes(x = va.select.y - va.ans.y)) + 
  geom_dots()  
```

Distribution of signed error: 

```{r}
#| code-fold: true

task3_df %>% ggplot(aes(x = va.select.y - va.ans.y)) + 
  geom_dots() + 
  geom_vline(xintercept = 0, linetype="dashed", color="black") + 
  labs(title="Task 3 -- CDF Median") 
```

Relationship between distance from the y-axes and the signed distribution: 

```{r}
task3_df %>% ggplot(aes(x = va.ans.x, y = va.select.y - va.ans.y)) + 
  geom_point(alpha = 0.5) + 
  geom_hline(yintercept = 0, linetype="dashed", color="black") + 
  labs(title="Task 3 -- CDF Median") 
```

The model we fit is going to be very similar to the ones we fitted for task 5. 

$$
\begin{aligned}
\text{error}_{\text{va}} &\sim \mathcal{N}(0, \sigma[i]) \\ 
\sigma[i] &= \texttt{va.ans.x}[i] \times \hat{\sigma}[i] \\ 
\hat{\sigma}[i] & \sim \mathcal{N}(0, 1)
\end{aligned}
$$

## Model 1 --- Visual Angle

```{r}
f <- bf(error_va ~ 0, 
        sigma ~ 0 + offset(log(va.ans.x)) + (1 | participantId))
p <- prior(constant(1), class="sd", group="participantId", dpar="sigma")
```

Fit the model: 

```{r}
#| label: task3-fit-model
#| output: false 

m3.1 <- task3_df %>% 
  mutate(error_va = va.select.y - va.ans.y) %>% 
  mutate(error_va = ifelse(abs(error_va) < tolerance, 0, error_va)) %>% 
  brm( 
    formula = f, 
    data = ., 
    family = gaussian(),
    prior = p,
    control = list(adapt_delta = 0.95),
    chains = 4, 
    iter = 6000, 
    file="models/m3.1",
    save_pars = save_pars(all=TRUE)
)
```

```{r}
summary(m3.1)
plot(m3.1)
pp_check(m3.1, ndraws = 100)
```

Get a sense of the $\hat{\sigma}[i]$: 

```{r}
m3.1 %>% spread_draws(r_participantId__sigma[participantId, term]) %>% 
  filter(term == "Intercept") %>% 
  mutate(hat_sigma = exp(r_participantId__sigma)) %>% 
  ggplot(aes(y = participantId, x = hat_sigma)) + 
  # stat_pointinterval() 
  stat_halfeye()
```

## Model 2 --- Physical space

```{r}
f <- bf(error_phy ~ 0, 
        sigma ~ 0 + offset(log(phy.ans.x + dist_to_screen))+ (1 | participantId))
p <- prior(constant(1), class = "sd", group = "participantId", dpar="sigma")
```

Fit the model: 

```{r}
#| label: task3-fit-model-2
#| output: false 

m3.2 <- task3_df %>% 
  mutate(error_phy = phy.select.y - phy.ans.y) %>% 
  mutate(error_phy = ifelse(abs(error_phy) < tolerance, 0, error_phy)) %>%
  brm( 
    formula = f, 
    data = ., 
    family = gaussian(),
    prior = p,
    control = list(adapt_delta = 0.95),
    chains = 4, 
    iter = 6000, 
    file="models/m3.2",
    save_pars = save_pars(all = TRUE)
)
```

Plot the results: 

```{r}
summary(m3.2)
plot(m3.2)
pp_check(m3.2, ndraws = 100)
```

### Compare the two models 

```{r}
m3.1 <- add_criterion(m3.1, "loo", recompile=TRUE)
m3.2 <- add_criterion(m3.2, "loo", moment_match = TRUE, recompile=TRUE)

loo_compare(m3.1, m3.2)
```

Again, the visual angle model seems better. 

# Task 1 -- Split area into equal halves

<!-- Hmm is this an area judgment or a ratio judgment ...?  -->

Load data: 

```{r}
#| label: get task 1 data 
#| code-fold: true 

task1_df <- task_df %>% filter(task == "task1") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.select.left_area = psgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE), 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  left_join(participants, by = join_by(participantId))
```

Plotting selection, answer, and `x` coordinate for highest point against lambda: 

```{r}
task1_df %>% ggplot(aes(y = data.ans.x - data.select.x, x = param.lambda)) + 
  geom_point(alpha = 0.5) + 
  geom_point(aes(y = data.ans.x - param.mu, x = param.lambda), color = "red", alpha = 0.5)
```

As can be seen here, there is a clear difference in trend ... As skewness $\lambda$ changes, the difference between the median `data.ans.x` and the mode `param.mu` increases. This provides some evidence that we're not purely using the highest point as a proxy. 

Plotting left area, answer, as well as `answer^0.7`: 

```{r}
task1_df %>% ggplot(aes(x = param.lambda, y = data.select.left_area)) + 
  geom_point(alpha = 0.5) + 
  geom_hline(yintercept = 0.5, linetype="dashed", color = "gray") + 
  geom_hline(yintercept = 0.5^0.7, linetype = "dashed", color = "darkgray") + 
  ylim(0, 1)
```

Plotting other relations: 

```{r}
#| layout-ncol: 2
#| code-fold: true 


task1_df %>% ggplot(aes(x = param.lambda, y = data.select.x - param.mu)) + geom_point(alpha = 0.5)

task1_df %>% ggplot(aes(x =  data.select.x - param.mu)) + 
  geom_dots()

task1_df %>% ggplot(aes(y =  data.select.x - param.mu, x = dist_to_screen)) + 
  geom_point()

task1_df %>% ggplot(aes(y =  data.select.left_area - 0.5, x = dist_to_screen)) + 
  geom_point()

task1_df %>% ggplot(aes(x = data.ans.x - data.select.x)) + 
  geom_dots()

task1_df %>% ggplot(aes(y = data.ans.x - data.select.x, x = dist_to_screen)) + 
  geom_point()

task1_df %>% ggplot(aes(y = data.select.left_area^(1/0.7), x = param.lambda)) + 
  geom_point()
```

- There does not appear to be any trend with `dist_to_screen` ... the spread remains about the same size, regardless of how far / near we are from the screen ... 
- According to Steven's law, we have that $S = I^{0.7}$, or, perceived sensation = stimuli intensity `^ 0.7`. It can be seen from this example however that the shape clearly impacts area perception. 
  - [ ] we could try to only sample from $|\lambda| \in [0.4, 0.8]$ and see what the result looks like 
  - [ ] instead of trying to sample $\lambda$ from a Gaussian we could try to sample it from something more evenly distributed ... 
  - [ ] we could also make this task about finding the split where left area = 0.4 or left_area = 0.6 or something like that ... 
- Is this where we use the cyclical power model to fit it ...?


# Task 2 -- Find highest point on curve 

First we get the data: 

```{r}
#| label: get task 2 data
#| code-fold: true 

task2_df <- task_df %>% filter(task == "task2") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.ans.x = param.mu, 
         data.ans.y = dsgt(param.mu, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE),
         data.select.grad = numDeriv::grad(dsgt, data.select.x, mu = param.mu, sigma = param.sigma, lambda = param.lambda, p = param.p, q = param.q, mean.cent = FALSE), 
         data.select.angle = atan(data.select.grad) * 180 / pi) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  # calculate things related to participants' selection 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM),
         va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>% 
  # calculate things related to answer 
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x), 
         pixel.ans.y = data_to_pixel_y(data.ans.y), 
         phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM), 
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM),
         va.ans.x = vis_angle(phy.ans.x, dist_to_screen),
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen))
```

Some preliminary checks of relations: 

```{r}
#| layout-ncol: 2

task2_df %>% ggplot(aes(y = phy.ans.y - phy.select.y, x = dist_to_screen)) + 
  geom_point()

task2_df %>% ggplot(aes(y = va.ans.y - va.select.y, x = dist_to_screen)) + 
  geom_point()

task2_df %>% ggplot(aes(x = va.ans.y - va.select.y)) + geom_dots()
task2_df %>% ggplot(aes(x = phy.ans.y - phy.select.y)) + geom_dots()
```

## Model 1 -- Vis Angle space

Based on our prior assumptions so far, the vis angle space model should not include dist_to_screen but the physical model should. Therefore, a potential model looks as follows: 

$$
\begin{aligned}
\text{error}_{\text{va}}[i] \sim \text{Half-Normal}(\sigma[i]) \\ 
\sigma[i] \sim \text{Log-Normal}(0, 1)
\end{aligned}
$$

Specify formula: 

```{r}
#| label: task 2 formula

f <- bf(error_va | trunc(lb = 0) ~ 0, 
        sigma ~ 0 + (1 | participantId), 
        family = gaussian())

# get prior 
task2_df %>% mutate(error_va = va.select.x - va.ans.x) %>%
get_prior(formula = f, 
          data = .)

p <- prior(lognormal(0, 1), class = "sd", dpar="sigma", group="participantId")
```

:::{.caution}
While doing the fitting I found some issues ... For task 2, `error_va = va.ans.y - va.select.y` should always be non-negative, but I got some negative values that are very, very small. I am setting a tolerance of `1e-12` --- any value smaller than this range will be regarded as 0. See the following for the negative values 

```{r}
task2_df %>% mutate(error_va = va.ans.y - va.select.y) %>% filter(error_va < 0) %>% 
  select(data.select.y, data.ans.y) %>% 
  mutate(diff = data.select.y - data.ans.y) %>% 
  pull(diff)
```

:::

Fit the model: 

```{r}
#| output: false
#| label: task2-fit-model

m2.1 <- task2_df %>% mutate(error_va = va.ans.y - va.select.y) %>% 
  mutate(error_va = ifelse(abs(error_va) < 1e-8, 0, error_va)) %>% 
  brm(formula = f,
      data = ., 
      prior = p,
      control = list(adapt_delta = 0.99), # note this is higher than 0.95
      chains = 4, 
      iter = 8000, # more iterations
      warmup = 2000, # higher than usual 
      file="models/m2.1", 
      save_pars = save_pars(all=TRUE)
      )
```

Let's see some fitted results: 

```{r}
summary(m2.1)
plot(m2.1)
# posterior predictive check 
pp_check(m2.1, ndraws = 100)
```

## Model 2 -- Physical

Math model: 

$$
\begin{aligned}
\text{error}_{\text{phy}}[i] &\sim \text{Half-Normal}(\sigma[i]) \\ 
\sigma[i] &= \texttt{dist_to_screen} \times \hat{\sigma}[i]\\ 
\hat{\sigma}[i] &\sim \text{Log-Normal}(0, 1)
\end{aligned}
$$

Formula: 

```{r}
f <- bf(error_phy | trunc(lb = 0) ~ 0, 
        sigma ~ 0 + offset(log(dist_to_screen)) + (1 | participantId), 
        family = gaussian())

p <- prior(lognormal(0, 1), class = "sd", dpar="sigma", group="participantId")
```

Fit model: 

```{r}
#| output: false 

m2.2 <- task2_df %>% 
  mutate(error_phy = phy.ans.y - phy.select.y) %>% 
  # note that the usual tolerance of 1e-12 is not good enough 
  # mutate(error_phy = ifelse(abs(error_phy) < 1e-8, 0, error_phy)) %>% 
  # let's just make everything negative to be 0 
  mutate(error_phy = ifelse(error_phy < 0, 0, error_phy)) %>% 
  brm(formula = f,
      data = ., 
      prior = p,
      control = list(adapt_delta = 0.99), # note this is higher than 0.95
      chains = 4, 
      iter = 8000, # more iterations
      warmup = 2000, # higher than usual 
      file="models/m2.2", 
      save_pars = save_pars(all=TRUE)
      )
```

Check fit: 

```{r}
summary(m2.2)
plot(m2.2)
pp_check(m2.2, ndraws = 100)
```

## Compare model 1 and model 2 

```{r}
m2.1 <- add_criterion(m2.1, "loo", moment_match=TRUE, recompile=TRUE)
m2.2 <- add_criterion(m2.2, "loo", moment_match = TRUE, recompile=TRUE)

loo_compare(m2.1, m2.2)
```

The visual angle version is also better. 

---

Then we check to make sure that things are correct: 

```{r}
#| output: false 
#| code-fold: true 
#| label: task 2 verify 

task2_df %>% ggplot(aes(x = data.select.x - data.ans.x)) + 
  geom_dots()

task2_df %>% ggplot(aes(x = data.select.y - data.ans.y)) + 
  geom_dots()

task2_df %>% ggplot(aes(x = pixel.select.x - pixel.ans.x)) + 
  geom_dots()

task2_df %>% ggplot(aes(x = pixel.select.y - pixel.ans.y)) + 
  geom_dots()

task2_df %>% ggplot(aes(x = phy.select.x - phy.ans.x)) + 
  geom_dots()  

task2_df %>% ggplot(aes(x = phy.select.y - phy.ans.y)) + 
  geom_dots() 

task2_df %>% ggplot(aes(x = va.select.x - va.ans.x)) + 
  geom_dots()  

task2_df %>% ggplot(aes(x = va.select.y - va.ans.y)) + 
  geom_dots()  
```

Note that `grad.x` is calcultaing the angle of the slope^[i.e. gradient] at selection `x`. It is NOT the perceived angle. 

Let's plot the relation between the angle selected and the parameters: 

```{r}
#| code-fold: true 
#| layout-ncol: 2

task2_df %>% ggplot(aes(y = data.select.grad, x = param.p)) +
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = data.select.grad, x = param.q)) +
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = data.select.grad, x = param.lambda)) +
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = data.select.grad, x = param.mu)) +
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = data.select.grad, x = param.sigma)) +
  geom_point(alpha = 0.5)

task2_df %>% ggplot(aes(y = data.select.grad, x = dist_to_screen)) +
  geom_point(alpha = 0.5)
```

The only thing that is worth noting is the relationship between `data.select.grad` and `dist_to_screen`. What about the corresponding angle? 

```{r}
task2_df %>% ggplot(aes(y = data.select.angle, x = dist_to_screen)) +
  geom_point(alpha = 0.5)
```

---

How to calculate the actual angle, based on the selection? First we can check out the distribution of calculated angle (relative to the highest point?)

```{r}
task2_df %>% 
  mutate(diff.x = phy.select.x - phy.ans.x, 
         diff.y = phy.select.y - phy.ans.y, 
         angle = atan(diff.y / diff.x) * 180 / pi) %>% 
  ggplot(aes(x = angle)) + 
  geom_dots()
```

We are seeing these extreme outliers because `diff.x` is very, very small, almost close to 0. The above calculations does not deal well with very, very small denominators. Doing things in the visual angle space gives the same outliers. Let's just round things up to 3: 

```{r}
task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x, 
                    diff.y = phy.select.y - phy.ans.y) %>% 
  mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>% 
  mutate(angle = case_when(diff.x == 0 ~ 0, 
                           diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>% 
  ggplot(aes(x = angle)) + 
  geom_dots()

task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x, 
                    diff.y = phy.select.y - phy.ans.y) %>% 
  mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>% 
  mutate(angle = case_when(diff.x == 0 ~ 0, 
                           diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>% 
  ggplot(aes(x = angle, y = data.select.angle)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "dashed")
```

This just means that if we were to use the highest point and the selected point as ways to calculate the actual angle, then it is **larger** than the actual slope at the selected point. 

Is this newly calculated `angle` going to be related to `dist_to_screen`? 

```{r}
task2_df %>% mutate(diff.x = phy.select.x - phy.ans.x, 
                    diff.y = phy.select.y - phy.ans.y) %>% 
  mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>% 
  mutate(angle = case_when(diff.x == 0 ~ 0, 
                           diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>% 
  ggplot(aes(x = dist_to_screen, y = angle)) + 
  geom_point(alpha = 0.5)
```

Compare to the graph before, the difference is that this one has a bigger variance. 

What if we use the numbers in the visual angle space and not the physical space? 

```{r}
task2_df %>% mutate(diff.x = va.select.x - va.ans.x, 
                    diff.y = va.select.y - va.ans.y) %>% 
  mutate(diff.x = round(diff.x, 3), diff.y = round(diff.y, 3)) %>% 
  mutate(angle = case_when(diff.x == 0 ~ 0, 
                           diff.x != 0 ~ atan(diff.y / diff.x) * 180 / pi )) %>% 
  ggplot(aes(x = dist_to_screen, y = angle)) + 
  geom_point(alpha = 0.5)
```

Hmm doesn't seem to be much different, which is to be expected, as slope is unit-less. 

--- 

# Task 4 -- Find slope 

```{r}
#| label: get task 4 data 

task4_df <- task_df %>% filter(task == "task4") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.select.slope = dsgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE), 
         data.ans.slope = dsgt(param.mu, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE)) %>% 
  left_join(participants, by = join_by(participantId))
```

Let's first see the distribution of signed error: 

```{r}
task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point()
```

It seems that the selected slope is consistently smaller than the actual slope of the answer. 

Let's see if there's any obvious relation between signed error and parameters of the distribution: 

```{r}
#| layout-ncol: 2
#| code-fold: true 


task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.mu)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.sigma)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.lambda)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.p)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.q)) + 
  geom_point()
```

I don't see any apparent trends ... 