---
title: "brms-syntax"
format:
  html: 
    toc: true 
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
```

# Get simulation data 

Model: 

$$\begin{align*}
y_i &\sim \mathcal{N}(\alpha_{\text{PID}[i]}, \beta_{\text{PID}[i]}) \\ 
\alpha_j &\sim \mathcal{N}(5, 1), j \in [50] \\ 
\beta_j &\sim |\mathcal{N}(0, 2)|, j \in [50]
\end{align*}$$

Let's say we have 100 participants. 

```{r}
n <- 50
trials <- 20
intercept <- rnorm(n, mean = 5, sd = 1)
variance <- abs(rnorm(n, mean = 0, sd = 1))

data <- data.frame(PID = rep(1:n, each = trials), 
           mean = rep(intercept, each = trials), 
           variance = rep(variance, each = trials)) %>% 
   mutate(out = map2_dbl(mean, variance, ~rnorm(1, mean = .x, sd = .y)))

# get a sense of what data looks like 
data %>%
  ggplot(aes(x = out)) +
  geom_dots()
```

# Model 1 

```{r}
f <- bf(out ~ 1)

get_prior(f, data = data)
```

```{r}
#| output: false 
#| eval: false 
m1 <- brm(f, data = data, family = gaussian)
```

We can check the variables from the fitted model using `get_variables`:

```{r}
#| eval: false 
get_variables(m1)
```

There are a lot of variables here, and we only care about those that are before `lprior`, namely: 1) `b_Intercept`, 2) `sigma`, and 3) `Intercept`. 

To get a better sense of what's happening under the hood, it makes sense to check the stan code, which we can do as follows: 

```{r}
#| column: margin 
make_stancode(bf(out ~ 1), data = data, family = gaussian())
```

---

The above stan code has several blocks. Let's examine the non-empty ones one by one: 

```default
data {
  int<lower=1> N;  // total number of observations      # <1>
  vector[N] Y;  // response variable                    # <2>
  int prior_only;  // should the likelihood be ignored? # <3> 
}
```
1. The number of rows in `data`
2. The response variable, which is the `out` column in `data`, which has length `N`
3. A flag that tells `brms` whether we're doing a prior predictive check 


```default
parameters {
  real Intercept;  // temporary intercept for centered predictors # <4>
  real<lower=0> sigma;  // dispersion parameters                  # <4> 
}
```
4. Notice that these are scalar values, *NOT* vector values. They correspond to `coef = Intercept` and `coef = sigma` in the `get_prior` results. 

```default
transformed parameters {
  real lprior = 0;  // prior contributions to the log posterior 
  lprior += student_t_lpdf(Intercept | 3, 5.2, 2.5); # <5> 
  lprior += student_t_lpdf(sigma | 3, 0, 2.5)        # <6> 
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);            # <6> 
}
```
5. The default Intercept is drawn from `student_t(3, 5.2, 2.5)` 
6. The default `sigma`, with `lb = 0` 

```default 
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);     # <7>
    mu += Intercept;                       # <7> 
    target += normal_lpdf(Y | mu, sigma);  # <8> 
  }
  // priors including constants
  target += lprior;                        # <9>
}
```
7. Create a vector of length `N`, where each element is `Intercept`. 
8. The likelihood function, corresponds to $y_i \sim \mathcal{N}(\mu, \sigma)$ 
9. Hmm I'm actually confused cause this is different than how I'm implementing things ...

```default
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept; # <10>
}
```
10. This tells us that `b_Intercept` is just `Intercept` by construction. 


<!-- Check fitted results:  -->

<!-- ```{r} -->
<!-- summary(m1) -->
<!-- plot(m1) -->

<!-- m1 %>% spread_draws(b_Intercept, sigma, Intercept) %>% -->
<!--   pivot_longer(4:ncol(.)) %>%  -->
<!--   ggplot(aes(x = value, y = name)) +  -->
<!--   stat_halfeye() -->
<!-- ``` -->





<!-- From the above we know that by construction, `b_Intercept` = `Intercept`.  -->

<!-- # Model 1.1  -->

<!-- ```{r} -->
<!-- f <- bf(out ~ 0 + (1 | PID)) -->

<!-- get_prior(f, data = data) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| output: false  -->

<!-- m1.1 <- brm(f, data = data, family = gaussian) -->
<!-- get_variables(m1.1) -->
<!-- make_stancode(bf(out ~ 0 + (1 | PID)), data = data, family = gaussian()) -->
<!-- ``` -->

<!-- From the `get_variables` outcome, there are the things we care about:  -->
<!-- - `sd_PID__Intercept`: standard deviation  -->
<!-- - `sigma`: unclear ...  -->
<!-- - `r_PID[num, Intercept]`: intercept per participant  -->

<!-- Check output:  -->

<!-- ```{r} -->
<!-- summary(m1.1) -->
<!-- plot(m1.1) -->
<!-- ``` -->



# Model 2

```{r}
f <- bf(out ~ 1 + (1 | PID))

get_prior(f, data = data)
```

The middle three rows are the new ones compared to Model 1, when we're adding `(1|PID)`. 

```{r}
#| output: false
#| eval: false 

m2 <- brm(f, data = data, family = gaussian)
get_variables(m2)
```

`get_variables` give us:

- `b_Intercept`
- `sd_PID__Intercept` --- NEW
- `sigma`
- `Intercept`: the same as `b_Intercept` 
- `r_PID[num, Intercept]` --- NEW 

Stan code:

```{r}
#| column: margin 
make_stancode(out ~ 1 + (1 | PID),
              data = data,
              family = gaussian)
```

:::{.callout}
### Non-centered parameterization 

In centered parameterization, you might write models like 
$$\begin{align*}
a_j &\sim \mathcal{N}(a_0, \sigma_0) \\ 
y &\sim \mathcal{N}(a_j, ...) 
\end{align*}$$

In non-centered parameterization, you write things like this instead: 
$$\begin{align*}
z_j &\sim \mathcal{N}(0, 1) \\ 
a_j &= a_0 + \sigma \times z_j \\ 
y &\sim \mathcal{N}(a_j, ...) 
\end{align*}$$
:::


Let's walk through this one, annotating only the new ones compared to what we had before: 

```default
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels                   # <1>
  int<lower=1> M_1;  // number of coefficients per level            # <2>
  array[N] int<lower=1> J_1;  // grouping indicator per observation # <3> 
  // group-level predictor values
  vector[N] Z_1_1;                                                  # <4>
  int prior_only;  // should the likelihood be ignored?             
}
```
1. The number of different levels in the group, `PID`. Which would be the number of participants. 
2. ??? 
3. This array is a mapping function, which tells us which participant this row is corresponding to. 
4. Unclear ... 

```default
parameters {
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // dispersion parameter
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations   # <5>
  array[M_1] vector[N_1] z_1;  // standardized group-level effects # <6>
}
```
5. Since `M_1 = 1`, `sd_1` is a vector of length 1, and we still have to access its value by `sd_1[1]`  
6. By the same logic, `z_1` is also a vector of length 1, and `z_1[1]` is a vector of length `N_1`.  

```default
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects                # <7> 
  real lprior = 0;  // prior contributions to the log posterior
  r_1_1 = (sd_1[1] * (z_1[1]));                                    # <8> 
  lprior += student_t_lpdf(Intercept | 3, 5.3, 2.5);
  lprior += student_t_lpdf(sigma | 3, 0, 2.5)
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)                       # <9> 
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
}
```
7. This is the centered parameterization of random effects. Think of this as $r_{\text{PID}[i]} = \sigma \times z_{\text{PID}[i]}$ where $z_j \sim \mathcal{N}(0, 1)$  
8. Recall that `sd_1[1]` is a scalar value, and `z_1[1]` is a vector of length `N_1`. 
9. `sd_1` is a vector, and `student_t_lpdf` handles this by computing the sum of log probabilities across all elements. Since there is only one element, `student_t_lpdf(sd_1 | ...)` is equivalent to `student_t_lpdf(sd_1[1] | ...)` 

```default
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);
    mu += Intercept;
    for (n in 1:N) {
      // add more terms to the linear predictor
      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];          # <10> 
    }
    target += normal_lpdf(Y | mu, sigma);
  }
  // priors including constants
  target += lprior;
  target += std_normal_lpdf(z_1[1]);              # <11> 
}
```
10. Recall that `J_1` is a vector of length `N`, which maps row to the participant it's associated with. REcall that `r_1_1` is a vector of length `N_1`, where `N_1` is the number of participants. `Z_1_1` of length `N`, is a **multiplier** for the random effect. In the most basic random intercept model, `Z_1_1` would be a vector of all 1s.   
11. They are doing some standardized tricks here ... 

--- 

If you do `summary(m2)` then it becomes 

```
Multilevel Hyperparameters:
~PID (Number of levels: 50) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)     1.04      0.11     0.85     1.29 1.01      335      781

Regression Coefficients:
          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept     5.23      0.14     4.96     5.50 1.03      167      362

Further Distributional Parameters:
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     0.91      0.02     0.87     0.95 1.00     2990     2879
```

I think `sd(Intercept)` corresponds to `sd_PID__Intercept`. 


<!-- # Model 3  -->

<!-- ```{r} -->
<!-- f <- bf(out ~ 1 + (1 | PID),  -->
<!--         sigma ~ 1) -->

<!-- get_prior(f, data = data) -->

<!-- m3 <- brm(f, data = data, family = gaussian) -->
<!-- ``` -->

# Model 4

```{r}
f <- bf(out ~ 1 + (1 | PID),
        sigma ~ 1 + (1 | PID))

get_prior(f, data = data)
```

```{r}
#| output: false
#| eval: false 
m4 <- brm(f, data = data, family = gaussian)
get_variables(m4)
```

`get_variables` give:

- `b_Intercept`
- `b_sigma_Intercept`
- `sd_PID__Intercept`
- `sd_PID__sigma_Intercept`
- `Intercept`
- `Intercept_sigma`
- `r_PID[num, Intercept]`
- `r_PID__sigma[num, Intercept]`

Stan code:

```{r}
#| column: margin 
make_stancode(bf(out ~ 1 + (1 | PID), 
                 sigma ~ 1 + (1 | PID)),
              data = data,
              family = gaussian)
```

Now let's break this one down: 

```default
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  array[N] int<lower=1> J_1;  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  // data for group-level effects of ID 2
  int<lower=1> N_2;  // number of grouping levels                    # <1> 
  int<lower=1> M_2;  // number of coefficients per level             # <2> 
  array[N] int<lower=1> J_2;  // grouping indicator per observation  # <3> 
  // group-level predictor values
  vector[N] Z_2_sigma_1;                                             # <4> 
  int prior_only;  // should the likelihood be ignored?
}
```
1. ... 
2. ... 
3. ... 
4. ... 

```default
parameters {
  real Intercept;  // temporary intercept for centered predictors
  real Intercept_sigma;  // temporary intercept for centered predictors # <5> 
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  array[M_1] vector[N_1] z_1;  // standardized group-level effects
  vector<lower=0>[M_2] sd_2;  // group-level standard deviations        # <6> 
  array[M_2] vector[N_2] z_2;  // standardized group-level effects      # <7> 
}
```
5. ... 
6. ... 
7. ... 

```default
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects
  vector[N_2] r_2_sigma_1;  // actual group-level effects       # <8>
  real lprior = 0;  // prior contributions to the log posterior 
  r_1_1 = (sd_1[1] * (z_1[1]));
  r_2_sigma_1 = (sd_2[1] * (z_2[1]));                           # <9> 
  lprior += student_t_lpdf(Intercept | 3, 5.3, 2.5);
  lprior += student_t_lpdf(Intercept_sigma | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)                    # <10> 
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)                    # <11> 
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
}
```
8. ... 
9. ... 
10. ... 
11. ... 

```default
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);
    // initialize linear predictor term
    vector[N] sigma = rep_vector(0.0, N);               # <12> 
    mu += Intercept;
    sigma += Intercept_sigma;                           # <13> 
    for (n in 1:N) {
      // add more terms to the linear predictor
      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];
    }
    for (n in 1:N) {
      // add more terms to the linear predictor
      sigma[n] += r_2_sigma_1[J_2[n]] * Z_2_sigma_1[n]; # <14> 
    }
    sigma = exp(sigma);                                 # <15> 
    target += normal_lpdf(Y | mu, sigma);              
  }
  // priors including constants
  target += lprior;
  target += std_normal_lpdf(z_1[1]);
  target += std_normal_lpdf(z_2[1]);
}
```
12. ... 
13. ... 
14. ... 
15. `sigma` is specified with a default `log` link, which is why we need to turn it to its right space 

---

If you do `summary(m4)` you get this: 

```
Multilevel Hyperparameters:
~PID (Number of levels: 50) 
                    Estimate Est.Error l-95% CI u-95% CI Rhat  Bulk_ESS Tail_ESS
sd(Intercept)           1.03      0.12     0.84     1.27 1.07        66      330
sd(sigma_Intercept)     1.45      0.14     1.20     1.77 1.02       162      355


Regression Coefficients:
                Estimate Est.Error l-95% CI u-95% CI Rhat   Bulk_ESS Tail_ESS
Intercept           5.21      0.14     4.91     5.50 1.08         69      154
sigma_Intercept    -0.92      0.19    -1.31    -0.56 1.03         61      156
               
Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).
```

where 

- `sd(Intercept)` corresponds to `sd_PID__Intercept`
- `sd(sigma_Intercept)` corresponds to `sd_PID__sigma_Intercept`
- `Intercept` corresponds to `b_Intercept`
- `sigma_Intercept` corresponds to `b_sigma_Intercept` 

