---
title: "mixture-model"
format: 
  html: 
    reference-location: margin
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
library(cmdstanr)
library(sgt)
```

# Read in data and all processing 

- All the things selected by the participants will be named `xxx.select.xxx` 
- All the things related to the actual answer will be named `xxx.ans.xxx`

```{r}
#| label: read-in-data
#| code-fold: true
#| output: false 

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)
```

```{r}
#| label: define-function
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| label: define-special-dfs
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```

# Task 1 --- split area into equal halves 

Load data: 

```{r}
#| label: get task 1 data 
#| code-fold: true 

task1_df <- task_df %>% filter(task == "task1") %>% 
  select(-slider.x, -slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  mutate(data.select.left_area = psgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE), 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  # pixel related 
  mutate(pixel.med.x = data_to_pixel_x(data.ans.x), 
         pixel.mod.x = data_to_pixel_x(param.mu)) %>% 
  # phy related 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.med.x = pixel_to_phy_x(pixel.med.x, pixelToMM), 
         phy.mod.x = pixel_to_phy_x(pixel.mod.x, pixelToMM)) %>% 
  # visual angle related 
  mutate(va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.med.x = vis_angle(phy.med.x, dist_to_screen), 
         va.mod.x = vis_angle(phy.mod.x, dist_to_screen))
```

Ok here's the math model^[Note that the notation for a standard mixture model focuses on specifying the probability **density**, which is different than McElreath's approach of specifying things in a more generative fashion ...]: 

$$\begin{align}
p_Y(y_i) & =  \lambda \cdot \Pr(\hat{y}^{\text{med}}_i) + (1 - \lambda) \cdot \Pr(\hat{y}^{\text{mod}}_i) \\
\hat{y}^{\text{med}}_i &\sim \mathcal{N}(y^\text{med}, (\sigma^\text{med}_{\text{PID}[i]})^2) \\ 
\hat{y}^{\text{mod}}_i &\sim \mathcal{N}(y^\text{mod}, (\sigma^{\text{mod}}_{\text{PID}[i]})^2) \\ 
(1) \,\, \lambda &\sim \text{Beta}(5, 5) 
\end{align}$$

From modeling task 2, it seems that $\hat{y}^\text{mod}$ does not really follow a normal distribution, but in fact follows something like this: 
$$\text{signed error} = \hat{y}^\text{mod}_i - y_i^\text{mod} \sim \text{Laplace}(\mu_{\text{PID}[i]}, \sigma_{\text{PID}[i]})$$


:::{.column-margin}
Perhaps a different way to write this in a generative fashion is: 
$$
\begin{align}
z_i &\sim \text{Categorical}(\pi_1, \pi_2) \\ 
y_i &\sim \mathcal{N}(\mu_{z_i}, \sigma_{z_i})
\end{align}
$$
where $z_i$ is the component indicator variable for observation $i$. 
:::

## Simple BRMS model

Let's try to write a simple brms model for this, note that right now $\lambda$ is just assumed to be drawn from Beta (or Dirichlet(1, 1)^[... which is going to be Uniform[0, 1] for the case of $K =2$])

```{r}
mix <- brms::mixture(gaussian, gaussian, order = "none")
formula <- bf(data.select.x ~ 0 + data.ans.x:mu1 + param.mu:mu2,
              mu1 ~ 0, 
              mu2 ~ 0, 
              sigma1 ~ 0 + (1 | participantId), 
              sigma2 ~ 0 + (1 | participantId)
              )
# get prior 
get_prior(formula = formula, data = task1_df, family = mix)

```

If we don't specify anything for `theta`, then theta is treated as drawn from a Dirichlet distribution, and theta values are assumed to be the same for all trials across all participants. 

Fit the model: 

```{r}
#| output: false 

mixture_model <- brm(
  formula = formula,
  data = task1_df, 
  family = mix, 
  # prior = prior, 
  chains = 4, 
  cores = 4, 
  iter = 10000, 
  warmup = 5000, 
  file = "models/task1", 
  control = list(adapt_delta = 0.95),
)
```

Check the fitted model: 

```{r}
summary(mixture_model)
pairs(mixture_model)
```

Let's look at the stan code of this: 

```{r}
make_stancode(formula = formula, 
              data = task1_df,
              family = mix)
```

Looking at the above sections^[termed `transformed parameters`, see [here](https://mc-stan.org/docs/reference-manual/blocks.html#program-block-transformed-parameters) for more details], we have the following: 

- `functions`: the place to define custom functions to be used in the model fitting process 
- `data`: declares all data that will be provided to th model from `R`. This includes observed variables, group identifiers, and any fixed values needed for the model. 
- `transformed data`: define deterministic transformations of data that only need to be computed once, before sampling begins. Useful for efficiency.
- `parameters`: the place to declare parameters that Stan will estimate. These are the unknowns in the model. 
- `model`: the core of the stan program, where we define the likelihood of the data given the parameters and specify the prior distributions. The `target` variable represents the **log posterior density** that Stan is trying to sample from. 


# Stan code 

## Same lambda for all participants

Everything is in **visual angle** space. 

$$
\begin{align}
p_Y(y_i) & =  \lambda \cdot \Pr(\hat{y}^{\text{med}}_i) + (1 - \lambda) \cdot \Pr(\hat{y}^{\text{mod}}_i) \\
\hat{y}^{\text{med}}_i &\sim \mathcal{N}(y^\text{med}, (\sigma^\text{med}_{\text{PID}[i]})^2) \\ 
\hat{y}^{\text{mod}}_i &\sim \mathcal{N}(y^\text{mod}, (\sigma^{\text{mod}}_{\text{PID}[i]})^2) \\ 
\lambda &\sim \text{Beta}(2, 2) 
\end{align}
$$

```{r}
model <- cmdstan_model("mix_models/m1.stan")
model$print()
```

:::{.caution-note}
Note that we are using the `log_mix` function instead of the `log_sum_exp` function, but they are equivalent for the case of two components. In the [documentation](https://mc-stan.org/docs/functions-reference/real-valued_basic_functions.html) we have 

$$
\begin{align}
\text{log_mix}(\theta, \lambda_1, \lambda_2) &= \log (\theta \exp(\lambda_1) + (1 - \theta) \exp(\lambda_2)) \\ 
&= \text{log_sum_exp}(\log (\theta) + \lambda_1, \log (1 - \theta) + \lambda_2)
\end{align}
$$
:::


```{r}
task1_df %>% ggplot(aes(x = va.med.x - va.select.x)) + 
  geom_dots()
```

Prepare data for stan: 

```{r}
N <- task1_df %>% nrow(.)
y <- task1_df %>% pull(va.select.x)
y_med <- task1_df %>% pull(va.med.x)
y_mod <- task1_df %>% pull(va.mod.x)
N_p <- task1_df %>% count(participantId) %>% nrow(.)
PID <- as.numeric(factor(task1_df$participantId))

stan_data <- list(
  N = N, 
  y = y, 
  y_med = y_med, 
  y_mod = y_mod, 
  N_p = N_p, 
  PID = PID
)
```

Fit the stan model^[there are several options here, one can use `rstan`, `brms`, and `cmdstanr`. I am going with `cmdstanr` here, as it always run wit the latest version of stan.]: 

```{r}
#| output: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4, 
  iter_warmup = 5000, 
  iter_sampling = 10000,
  adapt_delta = 0.99, 
  max_treedepth = 12
)

# save model 
fit$save_object(file = "mix_models/m1.rds")
```

The above gives 47% of divergent transitions, possibly because the prior is bad ...? 

Let's check the fitted result: 

```{r}
summary <- fit$summary()

# plot posterior summaries 
summary %>% filter(str_detect(variable, "lambda")) %>% 
  ggplot(aes(y = variable)) + 
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +  
  geom_point(aes(x = median)) + xlim(0.5, 1)
```

```{r}
# plot posterior summaries 
summary %>% filter(str_detect(variable, "sigma_m")) %>% 
  ggplot(aes(y = variable)) + 
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +  
  geom_point(aes(x = median)) + xlim(0, 1)
```

## Same lambda for same participant, different for different participants 

Math model: 

$$
\begin{align}
p_Y(y_i) & =  \lambda_{\text{PID}[i]} \cdot \Pr(\hat{y}^{\text{med}}_i) + (1 - \lambda_{\text{PID}[i]}) \cdot \Pr(\hat{y}^{\text{mod}}_i) \\
\hat{y}^{\text{med}}_i &\sim \mathcal{N}(y^\text{med}, (\sigma^\text{med}_{\text{PID}[i]})^2) \\ 
\hat{y}^{\text{mod}}_i &\sim \mathcal{N}(y^\text{mod}, (\sigma^{\text{mod}}_{\text{PID}[i]})^2) \\ 
\lambda_{\text{PID}[i]} &\sim \text{Beta}(2, 2) 
\end{align}
$$

Read in the stan model: 

```{r}
model <- cmdstan_model("mix_models/m2.stan")
```

Fit the model: 

```{r}
#| output: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4, 
  iter_warmup = 5000, 
  iter_sampling = 10000,
  adapt_delta = 0.99, 
  max_treedepth = 12
)

# save model 
fit$save_object(file = "mix_models/m2.rds")
```

42% transitions ended with a divergence ... 

```{r}
summary <- fit$summary()

# plot posterior summaries 
summary %>% filter(str_detect(variable, "lambda")) %>% 
  ggplot(aes(y = variable)) + 
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +  
  geom_point(aes(x = median)) + 
  xlim(0, 1)
 ```

```{r}
# plot posterior summaries 
summary %>% filter(str_detect(variable, "sigma_m")) %>% 
  ggplot(aes(y = variable)) + 
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +  
  geom_point(aes(x = median)) + xlim(0, 1)
```

# Stan code --- Laplace distribution 

Let's first write the math model: 

$$\begin{align*}
f(x) = \theta \cdot f_{\text{Laplace}}(x) + (1 - \theta) \cdot f_{\text{Gaussian}}(x)
\end{align*}$$

Read in stan code: 

```{r}
model <- cmdstan_model("mix_models/mixture.laplace.stan")
```

Prepare data for stan: 

```{r}
N <- task1_df %>% nrow(.)
x_select <- task1_df %>% pull(va.select.x)
x_med <- task1_df %>% pull(va.med.x)
x_mod <- task1_df %>% pull(va.mod.x)
J <- task1_df %>% count(participantId) %>% nrow(.)
PID <- as.numeric(factor(task1_df$participantId))

stan_data <- list(
  N = N, 
  x_select = x_select, 
  x_med = x_med, 
  x_mod = x_mod, 
  J = J, 
  PID = PID
)
```

Fit the model: 

```{r}
#| output: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4, 
  iter_warmup = 2000, 
  iter_sampling = 2000,
  adapt_delta = 0.99, 
  max_treedepth = 15
)

# save model 
fit$save_object(file = "mix_models/m4.rds")
```

