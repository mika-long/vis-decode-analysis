---
title: "mixture-model"
format: 
  html: 
    reference-location: margin
    toc: true
    code-overflow: wrap
    grid: 
      margin-width: 450px 
---

```{r}
#| echo: false
#| output: false

library(tidyverse)
library(ggplot2)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
library(cmdstanr)
library(sgt)
```

# Read in data and all processing 

- All the things selected by the participants will be named `xxx.select.xxx` 
- All the things related to the actual answer will be named `xxx.ans.xxx`

```{r}
#| code-fold: true
#| output: false 

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)

# create a dataframe for single participant (Sheng only) 
not_sheng <- c("5f37a06e-50e4-4489-948f-c4f25bd38d17", "85349f2b-c75a-46ff-8f80-fafc92da11a7")
single_pid_df <- task_df %>% filter(!participantId %in% not_sheng)
```

```{r}
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```

# Task 1 --- split area into equal halves 

Load data: 

```{r}
#| code-fold: true 

task1_df <- single_pid_df %>% filter(task == "task1") %>% # task_df
  select(-slider.x, -slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  mutate(data.select.left_area = psgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE), 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  # pixel related 
  mutate(pixel.med.x = data_to_pixel_x(data.ans.x), 
         pixel.mod.x = data_to_pixel_x(param.mu)) %>% 
  # phy related 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.med.x = pixel_to_phy_x(pixel.med.x, pixelToMM), 
         phy.mod.x = pixel_to_phy_x(pixel.mod.x, pixelToMM)) %>% 
  # visual angle related 
  mutate(va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.med.x = vis_angle(phy.med.x, dist_to_screen), 
         va.mod.x = vis_angle(phy.mod.x, dist_to_screen))
```

**Note that unless specifically mentioned, all estimations are done in visual angle measures.** 

# Single participant 

Math model: 

$$\begin{align*}
p_X(x_i) & =  \lambda \cdot \Pr(\hat{x}^{\text{med}}_i) + (1 - \lambda) \cdot \Pr(\hat{x}^{\text{mod}}_i) \\
\hat{x}^{\text{med}}_i &\sim \mathcal{N}(x^\text{med}, (\sigma^\text{med})^2) \\ 
\hat{x}^{\text{mod}}_i &\sim \text{Laplace}(x^\text{mod}, (\sigma^{\text{mod}})^2) \\ 
(1) \,\, \lambda &\sim \text{Beta}(5, 5) 
\end{align*}$$

## Same lambda 

If we set $\lambda \sim \text{Beta}(5, 5)$ then the sampling is more concentrated around 0.5, and if we set $\lambda \sim \text{Beta}(2, 2)$ then the sampling is closer to a uniform distribution over [0, 1]. 

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.1.stan")
# model$print()
model 
```

Prepare data for stan: 

```{r}
#| eval: false 
N <- task1_df %>% nrow(.)
x <- task1_df %>% pull(va.select.x)
x_med <- task1_df %>% pull(va.med.x)
x_mod <- task1_df %>% pull(va.mod.x)

stan_data <- list(
  N = N, 
  x = x, 
  x_med = x_med, 
  x_mod = x_mod
)
```

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.1.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check <- function(fitted_model) {
  y_rep <- fitted_model$draws("y_rep", format = "matrix")
  y <- fitted_model$draws("x_org", format = "matrix")[1, ] %>% as.vector(.)
  bayesplot::ppc_dens_overlay(y, y_rep[1:100, ])
}

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`lambda`) %>%
  ggplot(aes(x = lambda)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.1.rds")
```

## Different lambda --- varies by row 

Everything about the previous math model is the same, except for the part related to $\lambda$. 

### lambda = inv-logit(beta * (med - mod))

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.1.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.1.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```


Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.1.rds")
```

### lambda = inv-logit(beta * abs(med - mod))

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.2.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.2.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```


Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.2.rds")
```

### lambda = inv-logit(beta * (med - mod)^2)

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.3.stan")
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.3.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```


Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.3.rds")
```

### Compare all models 

```{r}
m1 <- readRDS("mix_models/sheng.mix.2.1.rds") # diff
m1_loo <- m1$loo()
m2 <- readRDS("mix_models/sheng.mix.2.2.rds") # abs diff
m2_loo <- m2$loo()
m3 <- readRDS("mix_models/sheng.mix.2.3.rds") # diff^2
m3_loo <- m3$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo, "Model 3" = m3_loo),
                  method = c("stacking"))
```

The second model seems to be fitting well. 

```{r}
m4 <- readRDS("mix_models/sheng.mix.1.rds")
m4_loo <- m4$loo()

brms::loo_model_weights(list("Model 4" = m4_loo, "Model 2" = m2_loo), method = c("stacking"))
```

It also performs better than the first one where we had a single lambda. 

Let's look at all of the other lambdas: 

```{r}
m2 <- readRDS("mix_models/sheng.mix.2.2.rds") # abs diff
summary <- m2$summary()

summary %>% filter(str_detect(variable, "^lambda")) %>%
  arrange(median) %>% 
  ggplot(aes(y = reorder(variable, median))) +
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +
  geom_point(aes(x = median))
```

One thing about the above model is that `lambda` is constrained to be >= 0.5. This is because of how we defined the function. We could also fit models that have an intercept, allowing the full range to be something more broad. 

## lambda = inv-logit (alpha + beta * (x.med - x.mod))

Read stan model: 

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.4.stan")
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.4.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.4.rds")
```

## lambda = inv-logit(alpha + beta * abs(x.med - x.mod))

Read stan model: 

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.5.stan")
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.5.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$code()
```



Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.5.rds")
```

## lambda = inv-logit(alpha + beta * square(x.med - x.mod))

```{r}
#| eval: false 
model <- cmdstan_model("mix_models/sheng.mix.2.6.stan")
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  parallel_chains = 4)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("mix_models/sheng.mix.2.6.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "mix_models/sheng.mix.2.6.rds")
```

```{r}
task1_df %>% 
```



## Compare all six models 

```{r}
m1 <- readRDS(file = "mix_models/sheng.mix.2.1.rds")
m2 <- readRDS(file = "mix_models/sheng.mix.2.2.rds")
m3 <- readRDS(file = "mix_models/sheng.mix.2.3.rds")
m4 <- readRDS(file = "mix_models/sheng.mix.2.4.rds")
m5 <- readRDS(file = "mix_models/sheng.mix.2.5.rds")
m6 <- readRDS(file = "mix_models/sheng.mix.2.6.rds")
# calculate loo 
m1_loo <- m1$loo()
m2_loo <- m2$loo()
m3_loo <- m3$loo()
m4_loo <- m4$loo()
m5_loo <- m5$loo()
m6_loo <- m6$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo, "Model 3" = m3_loo, 
                             "Model 4" = m4_loo, "Model 5" = m5_loo, "Model 6" = m6_loo),
                  method = c("stacking"))
```

So the model with the intercept is apparently better than the model without the intercept. Let's see what the lambda values look like for `m5`: 

```{r}
summary <- m5$summary()

summary %>% filter(str_detect(variable, "^lambda")) %>%
  arrange(median) %>% 
  ggplot(aes(y = reorder(variable, median))) +
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +
  geom_point(aes(x = median))
```

Hmm with variance this big, what are we actually learning ...? 

```{r}
task1_df %>% ggplot(aes(x  = param.lambda, y = va.select.x - va.med.x)) + geom_point()
```

```{r}
fit$code()
```

```{r}
as_draws_rvars(fit)$y_rep

task1_df %>% mutate(yrep = as_draws_rvars(fit)$y_rep) %>% 
  # turn this into long format data frame 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = yrep - va.med.x)) + stat_lineribbon()
```

```{r}

as_draws_rvars(fit)$lambda

m2 <- readRDS("mix_models/sheng.mix.2.2.rds") # abs diff
summary <- m2$summary()

summary %>% filter(str_detect(variable, "^lambda")) %>%
  arrange(median) %>% 
  ggplot(aes(y = reorder(variable, median))) +
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +
  geom_point(aes(x = median))


task1_df %>% mutate(mix_paramter = as_draws_rvars(fit)$lambda) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = mix_paramter)) + 
  stat_ribbon()

m2$code()
```

# Different participants 

Math model: 

$$\begin{align*}
p_X(x_i) & =  \lambda \cdot \Pr(\hat{x}^{\text{med}}_i) + (1 - \lambda) \cdot \Pr(\hat{x}^{\text{mod}}_i) \\
\hat{x}^{\text{med}}_i &\sim \mathcal{N}(x^\text{med}_i, (\sigma^\text{med}_{\text{PID}[i]})^2) \\ 
\hat{y}^{\text{mod}}_i &\sim \text{Laplace}(x^\text{mod}_i, (\sigma^{\text{mod}}_{\text{PID}[i]})^2) \\ 
(*) \,\, \lambda &\sim \text{Beta}(2, 2) 
\end{align*}$$
