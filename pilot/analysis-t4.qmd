---
title: "Analysis - Task 4"
format: 
  html: 
    code-fold: false
    reference-location: margin
    df-print: paged
    toc: true
    code-overflow: wrap
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
library(sgt)
library(numDeriv)
library(ggdist)
# setting the theme 
theme_set(theme_minimal())
# model
library(brms)
# plot results 
library(tidybayes)
```

# Set up

Read in data 

```{r}
#| label: read-in-data
#| code-fold: true

df1 <- read.csv("pilot-01.csv") %>% as_tibble(.)

df <- read.csv("pilot-02.csv") %>% as_tibble(.) %>% 
  bind_rows(df1)

exclude_participants <- c("67fd4f955123c6d79ae66a3a", "67eedf7be13ffb77c677d4dd", "6443ebb84fc33e703937a6f9")

task_df <- df %>% filter(status == "completed") %>% 
  select(participantId, trialId, responseId, answer) %>%
  filter(str_detect(trialId, "task")) %>%
  mutate(answer = as.numeric(answer)) %>% 
  pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  filter(type != "train") %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y) %>% 
  filter(!participantId %in% exclude_participants)
```

```{r}
#| label: define-function
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| label: define-special-dfs
#| code-fold: true

p <- df %>% filter(status == "completed") %>% 
  select(participantId, trialId, responseId, answer) %>% 
  filter(grepl("calibration", trialId))

p %>% filter(responseId == "ball-positions") %>% 
  select(participantId, answer) %>% 
  mutate(answer = gsub("\\[|\\]", "", answer)) %>% 
  mutate(split_answer = str_split(answer, ",")) %>% 
  mutate(numeric_vectors = map(split_answer, ~ as.numeric(.x))) %>% 
  unnest(split_answer) %>% 
  mutate(split_answer = as.numeric(split_answer)) %>% 
  group_by(participantId) %>% 
  summarise(avg.ball.pos = sum(split_answer) / 5, 
            min.ball.pos = min(split_answer),
            max.ball.pos = max(split_answer)) -> avg_ball_pos

avg_ball_pos %>% mutate(true_ball_pos = (avg.ball.pos * 5 - min.ball.pos - max.ball.pos)/3) %>% 
  select(participantId, true_ball_pos) -> true_ball_pos

participants <- p %>% filter(grepl("pixelsPerMM|dist-calibration-MM", responseId)) %>%
  select(-trialId) %>%
  mutate(answer = as.numeric(answer)) %>%
  pivot_wider(names_from = responseId, values_from = answer) %>%
  rename(pixelToMM = pixelsPerMM, dist_to_screen = `dist-calibration-MM`) %>%
  filter(!participantId %in% exclude_participants) %>%
  left_join(avg_ball_pos) %>% 
  mutate(phy = dist_to_screen * tan(13.5 * pi / 180), 
         px = phy * pixelToMM, 
         square_pos = round(px + avg.ball.pos))

participants %>% left_join(true_ball_pos) %>% 
  mutate(ball.square.dist = (square_pos - true_ball_pos) / pixelToMM, 
         d = ball.square.dist / tan(13.5 * pi / 180)) %>% 
  select(participantId, pixelToMM, d) %>% 
  rename(dist_to_screen = d) -> participants
```

# Task 4 --- find highest slope 

```{r}
#| label: get task 4 data 

epsilon <- 1e-10

data_to_screen_slope <- function(k){
  return(k * 395 / 53.5)
}

task4_df <- task_df %>% filter(task == "task4") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.select.slope = dsgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE), 
         data.ans.slope = dsgt(param.mu, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE)) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM),
         va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>%
  mutate(data.ans.x = param.mu, 
         data.ans.y = 1 / 2 * (psgt(data.ans.x + epsilon, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE) +  
                                            psgt(data.ans.x - epsilon, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE))) %>%
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x), 
         pixel.ans.y = data_to_pixel_y(data.ans.y)) %>% 
  mutate(phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM), 
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM)) %>% 
  mutate(va.ans.x = vis_angle(phy.ans.x, dist_to_screen), 
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen)) 

```

Check relations 

```{r}
task4_df %>% ggplot(aes(x = va.ans.x - va.select.x)) + geom_dots()
task4_df %>% ggplot(aes(x = va.ans.y - va.select.y)) + geom_dots()
task4_df %>% ggplot(aes(x = atan(va.ans.y / va.ans.x) * 180 / pi)) + geom_dots()
task4_df %>% ggplot(aes(x = va.ans.y / va.ans.x, y = data.ans.slope)) + 
  geom_point() + coord_equal()

task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()

task4_df %>% filter(data.select.slope - data.ans.slope == 0)

task4_df %>% ggplot(aes(x = atan(data.select.slope) * 180 / pi - atan(data.ans.slope) * 180 / pi)) + 
  geom_dots()

task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) + 
  geom_dots()

task4_df %>% ggplot(aes(x = atan(data_to_screen_slope(data.select.slope)) * 180 / pi - atan(data_to_screen_slope(data.ans.slope)) * 180 / pi)) + 
  # geom_dots()
  stat_ecdf()
# maybe try to fit a model here 
```

If we map the mathematical slope to the actual slope, this is what we get: 

- y axis range [0,1] correspond to 395 px -> 395px / unit 
- x axis range [-5, 5] correspond to 535 px -> 53.5 px / unit 

```{r}
task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope), 
                        y = data_to_screen_slope(data.ans.slope))) + 
  geom_point(alpha = 0.5) + 
  coord_equal()

task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) + 
  geom_dots()
```

Let's first see the distribution of signed error: 

```{r}
task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point(alpha = 0.5)
```

Let's see if there's any obvious relation between signed error and parameters of the distribution: 

```{r}
#| layout-ncol: 2
#| code-fold: true 
#| output: false 

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.mu)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.sigma)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.lambda)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.p)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.q)) + 
  geom_point()

task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()
```

## Exponential Model 

Math model: 

$$\begin{align*}
\text{error_i} &\sim \text{Exponential}(\lambda_{\text{PID}[i]}) \\ 
\log(\lambda_j) &\sim \mathcal{N}(\mu_\lambda, \sigma_\lambda) \\ 
\mu_\lambda &\sim \mathcal{N}(0, 1) \\ 
\sigma_\lambda &\sim \text{Half-Cauchy}(0, 1) 
\end{align*}$$

Formula: 

```{r}
f <- bf(error ~ 0,
        mu ~ (1 | participantId),  # this is actually lambda 
        family = exponential())

# mu has log-link, so we don't pose any bounds on it 
p <- c(
  prior(normal(0, 1), class = Intercept), 
  prior(cauchy(0, 1), class = sd, group = participantId, lb = 0)
)
```

```{r}
#| eval: false 
#| echo: false 

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>% 
  get_prior(f, data = .)

task2_df %>% mutate(error.x = va.select.x - va.ans.x) %>% 
  make_stancode(f, data = ., prior = p)
```

Fir the model: 

```{r}
#| output: false 
#| label: fit exponential 


model.exponential <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(abs(error) < tolerance, tolerance, error)) %>% # brms need rsp > 0 
  brm(formula = f, 
      data = ., 
      prior = p, 
      chains = 4, 
      file = "models/task4.exponential", 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE)
  )
```

Check fit: 

```{r}
summary(model.exponential)
```

```{r}
plot(model.exponential)
pp_check(model.exponential, ndraws = 100)
pp_check(model.exponential, ndraws = 100, type = "pit_ecdf") # pet Matt's suggestion 
```

See is actually not bad at all ... 

```{r}
model.exponential %>% get_variables()

model.exponential %>% spread_draws(b_Intercept, sd_participantId__Intercept) %>% 
  mutate(lambda = exp(b_Intercept))  %>% 
  pivot_longer(5:6)  %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(abs(error) < tolerance, tolerance, error)) %>% 
  add_predicted_draws(model.exponential, ndraws = 100)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "skyblue", alpha = 0.8)  + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)))
```

## Lomax Model 

Lomax is Pareto Type II distribution. We had similar models for task 2. 

```{r}
lomax_family <- custom_family(
  "lomax",
  dpars = c("mu", "lambda", "alpha"),    # includes mu as required
  links = c("identity", "log", "log"),   # appropriate links
  lb = c(-Inf, 0, 0),                    # appropriate bounds
  type = "real"
)

stan_funs <- "
  real lomax_lpdf(real y, real mu, real lambda, real alpha) {
    return pareto_type_2_lpdf(y | 0, lambda, alpha);  # mu is ignored
  }
  
  real lomax_rng(real mu, real lambda, real alpha) {
    return pareto_type_2_rng(0, lambda, alpha);  # mu is ignored
  }
"
```

```{r}
# Define the formula for the model
f <- bf(
  error ~ 0,  
  mu ~ 0, 
  alpha ~ (1 | participantId), 
  lambda ~ (1 | participantId)  
)

p <- c(
    prior(normal(0, 1), class = Intercept, dpar = alpha),
    prior(cauchy(0, 1), class = sd, group = participantId, dpar = alpha, lb = 0),
    prior(normal(0, 1), class = Intercept, dpar = lambda), 
    prior(cauchy(0, 1), class = sd, group = participantId, dpar = lambda, lb = 0)
  )
```

```{r}
#| eval: false 
#| echo: false 

task4_df %>%  mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% 
  get_prior(formula = f, family = lomax_family)

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope)) + epsilon) %>% 
  make_stancode(formula = f, family = lomax_family, prior = p)
```

```{r}
#| output: false 
#| label: fit lomax 

model.lomax <- task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>% 
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% 
  brm(
    formula = f,
    data = .,
    family = lomax_family,
    prior = p,
    stanvars = stanvar(scode = stan_funs, block = "functions"),
    chains = 4,
    file = "models/task4.lomax", 
    # init = list()
    control = list(adapt_delta = 0.99), 
    iter = 8000, 
  )
```

Check fit: 

```{r}
# Examine model results
summary(model.lomax)
plot(model.lomax)
```

Let's check the posterior of alpha and lambda: 

```{r}
model.lomax %>% spread_draws(`^b_.*`, regex = TRUE) %>% 
  mutate(b_alpha = exp(b_alpha_Intercept), b_lambda = exp(b_lambda_Intercept)) %>% 
  pivot_longer(6:7) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye() + 
  facet_wrap(~name, scales = "free") + 
  geom_vline(xintercept = 1e-10, linetype = "dashed", color = "gray")
```

Define custom functions for `pp_check` and `loo`: 

```{r}
expose_functions(model.lomax, vectorize = TRUE)

# Define the posterior_predict function in R as per vignette
posterior_predict_lomax <- function(i, draws, ...) {
  alpha <- draws$dpars$alpha[i]
  lambda <- draws$dpars$lambda[i]
  # Use Stan's rng function indirectly via brms
  lomax_rng(0, lambda, alpha)
}

log_lik_lomax <- function(i, prep){
  alpha <- brms::get_dpar(prep, "alpha", i = i)
  lambda <- brms::get_dpar(prep, "lambda", i = i)
  y <- prep$data$Y[i]
  lomax_lpdf(y, 0, lambda, alpha)
}

pp_check(model.lomax, ndraws = 100) 
pp_check(model.lomax, ndraws = 100, type = "pit_ecdf")
```

Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  add_predicted_draws(model.lomax, ndraws = 100)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "lightblue", alpha = 1) + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) # + 
  # xlim(0, 5)
```

## Weibull distribution 

- [wikipedia](https://en.wikipedia.org/wiki/Weibull_distribution) 
- [stan](https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html#weibull-distribution) 
- it's also implemented in brms! So no need for a custom function. 

```{r}
f <- bf(
  error ~ 0,  
  mu ~ (1 | participantId), # log link 
  shape ~ (1 | participantId), # log link 
  family = weibull
)

p <- c(
    prior(normal(0, 1), class = Intercept),
    prior(cauchy(0, 1), class = sd, group = participantId, lb = 0),
    prior(normal(0, 1), class = Intercept, dpar = shape), 
    prior(cauchy(0, 1), class = sd, group = participantId, dpar = shape, lb = 0)
  )
```

```{r}
#| eval: false 
#| echo: false 

task4_df %>%  mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% 
  get_prior(formula = f)

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope)) + epsilon) %>% 
  make_stancode(formula = f, prior = p)
```

Fit the model: 

```{r}
#| output: false 
#| label: fit weibull 


model.weibull <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% # brms needs value to be > 0  
  brm(formula = f, 
      data = ., 
      prior = p, 
      chains = 4, 
      file = "models/task4.weibull", 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE), 
      
  )
```

Check fit and posterior: 

```{r}
summary(model.weibull)
plot(model.weibull)
pp_check(model.weibull, ndraws = 100)
pp_check(model.weibull, ndraws = 100, type = "pit_ecdf")
```

Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  add_predicted_draws(model.weibull, ndraws = 100)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "lightblue", alpha = 1) + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) + 
  xlim(0, 10)
```

## Gamma distribution 

```{r}
f <- bf(error ~ 0,
        mu ~ (1 | participantId), 
        shape ~ (1 | participantId), 
        family = Gamma())

p <- c(
  prior(normal(0, 1), class = Intercept), 
  prior(cauchy(0, 1), class = sd, group = participantId, lb = 0), 
  prior(normal(0, 1), class = Intercept, dpar = shape), 
  prior(cauchy(0, 1), class = sd, group = participantId, dpar = shape, lb = 0)
)
```

```{r}
#| echo: false 
#| eval: false 

task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  get_prior(data = ., formula = f)
```

```{r}
#| output: false 
#| label: fit gamma 

model.gamma <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% # brms needs value to be > 0  
  brm(formula = f, 
      data = ., 
      prior = p, 
      chains = 4, 
      file = "models/task4.gamma", 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE), 
      control = list(adapt_delta = 0.99), 
      iter = 4000
  )
```

Check fit and posterior: 

```{r}
summary(model.gamma)
plot(model.gamma)
pp_check(model.gamma, ndraws = 100)
pp_check(model.gamma, ndraws = 100, type = "pit_ecdf")
```

Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  add_predicted_draws(model.gamma, ndraws = 100)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "lightblue", alpha = 1) + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) + 
  xlim(0, 10)
```

## Log-Normal distribution 

```{r}
f <- bf(error ~ 0,
        mu ~ (1 | participantId), 
        sigma ~ (1 | participantId), 
        family = lognormal())

p <- c(
  prior(normal(0, 1), class = Intercept), 
  prior(cauchy(0, 1), class = sd, group = participantId, lb = 0), 
  prior(normal(0, 1), class = Intercept, dpar = sigma), 
  prior(cauchy(0, 1), class = sd, group = participantId, dpar = sigma, lb = 0)
)
``` 

```{r}
#| echo: false 
#| eval: false 

task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  get_prior(data = ., formula = f)
```

```{r}
#| output: false 
#| label: fit lognormal 

model.lognormal <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% # brms needs value to be > 0  
  brm(formula = f, 
      data = ., 
      prior = p, 
      chains = 4, 
      file = "models/task4.lognormal", 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE)
  )
```

Check fit and posterior: 

```{r}
summary(model.lognormal)
plot(model.lognormal)
pp_check(model.lognormal, ndraws = 100)
pp_check(model.lognormal, ndraws = 100, type = "pit_ecdf")
```

Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  add_predicted_draws(model.lognormal, ndraws = 100)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "lightblue", alpha = 1) + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) + 
  xlim(0, 10)
```

## Model Comparison

```{r}
loo(model.exponential, model.lomax, model.weibull, model.gamma, model.lognormal)
```

The weibull model performs the best.There is also the potential todo of log-normal and gamma ... perhaps we should try them all ...? 

```{r}
#| layout-ncol: 3

pp_check(model.exponential, ndraws = 100, type = "pit_ecdf")
pp_check(model.lomax, ndraws = 100, type = "pit_ecdf")
pp_check(model.weibull, ndraws = 100, type = "pit_ecdf")
```



