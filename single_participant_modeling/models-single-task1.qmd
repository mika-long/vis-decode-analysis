---
title: "mixture-model"
format: 
  html: 
    reference-location: margin
    toc: true
    code-overflow: wrap
    grid: 
      margin-width: 450px 
---

```{r}
#| echo: false
#| output: false

library(tidyverse)
library(ggplot2)
# setting the theme 
theme_set(theme_minimal())
options(ggplot2.discrete.fill = function() scale_fill_brewer(palette = "Blues"))
# model
library(brms)
# plot results 
library(tidybayes)
library(cmdstanr)
library(sgt)
library(posterior)
```

# Read in data and all processing 

- All the things selected by the participants will be named `xxx.select.xxx` 
- All the things related to the actual answer will be named `xxx.ans.xxx`

```{r}
#| code-fold: true
#| output: false 

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)

# create a dataframe for single participant (Sheng only) 
not_sheng <- c("5f37a06e-50e4-4489-948f-c4f25bd38d17", "85349f2b-c75a-46ff-8f80-fafc92da11a7")
single_pid_df <- task_df %>% filter(!participantId %in% not_sheng)
```

```{r}
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```

# Task 1 --- split area into equal halves 

Load data: 

```{r}
#| code-fold: true 

task1_df <- single_pid_df %>% filter(task == "task1") %>% # task_df
  select(-slider.x, -slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  mutate(data.select.left_area = psgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE), 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  # pixel related 
  mutate(pixel.med.x = data_to_pixel_x(data.ans.x), 
         pixel.mod.x = data_to_pixel_x(param.mu)) %>% 
  # phy related 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.med.x = pixel_to_phy_x(pixel.med.x, pixelToMM), 
         phy.mod.x = pixel_to_phy_x(pixel.mod.x, pixelToMM)) %>% 
  # visual angle related 
  mutate(va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.med.x = vis_angle(phy.med.x, dist_to_screen), 
         va.mod.x = vis_angle(phy.mod.x, dist_to_screen))
```

**Note that unless specifically mentioned, all estimations are done in visual angle measures.** 

```{r}
task1_df %>% ggplot(aes(x = param.lambda, y = va.select.x - va.med.x)) + 
  geom_point(alpha = 0.5)
```

# Mixture models 

## Same lambda for all rows 

Math model: 

$$\begin{align*}
f_X(x_i) & =  \theta \cdot f_{\hat{X}}(\hat{x}^{\text{med}}_i) + (1 - \theta) \cdot f_{\hat{X}}(\hat{x}^{\text{mod}}_i) \\
\hat{x}^{\text{med}}_i &\sim \mathcal{N}(x^\text{med}, (\sigma^\text{med})^2) \\ 
\hat{x}^{\text{mod}}_i &\sim \text{Laplace}(x^\text{mod}, (\sigma^{\text{mod}})^2) \\ 
\theta &\sim \text{Beta}(5, 5) 
\end{align*}$$

If we set $\lambda \sim \text{Beta}(5, 5)$ then the sampling is more concentrated around 0.5, and if we set $\lambda \sim \text{Beta}(2, 2)$ then the sampling is closer to a uniform distribution over [0, 1]. 

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.1.stan")
# model$print()
model 
```

Prepare data for stan: 

```{r}
#| eval: false 
N <- task1_df %>% nrow(.)
x <- task1_df %>% pull(va.select.x)
x_med <- task1_df %>% pull(va.med.x)
x_mod <- task1_df %>% pull(va.mod.x)

stan_data <- list(
  N = N, 
  x = x, 
  x_med = x_med, 
  x_mod = x_mod
)
```

Define custom posterior predictive check function: 

```{r}
#| label: custom pp_check 
# custom_pp_check <- function(fitted_model) {
#   y_rep <- fitted_model$draws("y_rep", format = "matrix")
#   y <- fitted_model$draws("x_org", format = "matrix")[1, ] %>% as.vector(.)
#   bayesplot::ppc_dens_overlay(y, y_rep[1:100, ])
# }

custom_pp_check <- function(
   fitted_model, 
   fun = bayesplot::ppc_dens_overlay, 
   ndraws = 100, 
   transform = identity,
   ...
 ) {
   y_rep <- as_draws_rvars(fitted_model)$y_rep |> 
     transform() |>
     as_draws_matrix()
   if (!is.null(ndraws)) {
     y_rep = y_rep |>
       merge_chains() |>
       resample_draws(ndraws = ndraws) 
   }
   y <- fitted_model$draws("x_org", format = "matrix")[1, ] |>
     as.vector() |>
     transform()
   fun(y, y_rep, ...)
 }
```


Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 

fit <- readRDS("models/task1.beta.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()
custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`lambda`) %>%
  ggplot(aes(x = lambda)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.beta.rds")
```

## Different lambda --- varies by row 

Everything about the previous math model is the same, except for the part related to $\lambda$. 

### lambda = inv-logit(beta * (med - mod))

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.1.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 

fit <- readRDS("models/task1.diff.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.diff.rds")
```

### lambda = inv-logit(beta * abs(med - mod))

```{r}
#| eval: false 

model <- cmdstan_model("stan_files/sheng.mix.2.2.1.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.abs.diff.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.abs.diff.rds")
```

### lambda = inv-logit(beta * (med - mod)^2)

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.3.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.sq.diff.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`beta`) %>%
  ggplot(aes(x = beta)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.sq.diff.rds")
```

### LOO Compare the above models so far 

```{r}
m1 <- readRDS("models/task1.diff.rds") # diff
m1_loo <- m1$loo()
m2 <- readRDS("models/task1.abs.diff.rds") # abs diff
m2_loo <- m2$loo()
m3 <- readRDS("models/task1.sq.diff.rds") # diff^2
m3_loo <- m3$loo()
m4 <- readRDS("models/task1.beta.rds") # beta 
m4_loo <- m4$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo, "Model 3" = m3_loo, "Model 4" = m4_loo),
                  method = c("stacking"))
```

Let's zoom in on what the lambdas have: 

```{r}
m2 <- readRDS("models/task1.abs.diff.rds") # abs diff
summary <- m2$summary()

summary %>% filter(str_detect(variable, "^lambda")) %>%
  arrange(median) %>% 
  ggplot(aes(y = reorder(variable, median))) +
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +
  geom_point(aes(x = median))
```

One thing about the above model is that the mixing parameter `theta` is constrained to be >= 0.5. This is because of how we defined the function. We could also fit models that have an intercept, allowing the full range to be something more broad. 

### lambda = inv-logit (alpha + beta * abs(x.med - x.mod))

Math model: 

$$\begin{align*}
p_X(x_i) & =  \theta \cdot \Pr(\hat{x}^{\text{med}}_i) + (1 - \theta) \cdot \Pr(\hat{x}^{\text{mod}}_i) \\
\hat{x}^{\text{med}}_i &\sim \mathcal{N}(x^\text{med}_i, (\sigma^\text{med})^2) \\ 
\hat{x}^{\text{mod}}_i &\sim \text{Laplace}(x^\text{mod}_i, (\sigma^{\text{mod}})^2) \\ 
\theta &= \text{logit}^{-1}(\alpha + \beta \times |x^{\text{med}}_i - x^{\text{mod}}_i| ) \\ 
\beta &\sim \mathcal{N}(0, 1), \alpha \sim \mathcal{N}(0, 1)
\end{align*}$$

Note: 

- The prior on `alpha` would greatly influence what the value of `alpha` would be. If `alpha ~ N(0, 1)` then posterior `alpha` would be around 2. 
- We will implement a way to scale `alpha`. 

Read stan model: 

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.4.stan")
model
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
  )
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.alpha.abs.diff.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.alpha.abs.diff.rds")
```

**If we put a standard prior on alpha, it is usually positive, meaning that we still can't go all the way to negative with alpha + beta * abs(diff).**

### lambda = inv-logit(alpha + beta * abs(x.med - x.mod) / var)

... where `var` is defined as $\sqrt{(\sigma^\text{med})^2 + 2 \times \sigma^\text{mod}^2}$. 

Read stan model: 

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.5.stan")
model 
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
  )
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.alpha.abs.adj.diff.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.alpha.abs.adj.diff.rds")
```

### Compare between abs(diff) and abs(diff)/var 

```{r}
m1 <- read_rds("models/task1.alpha.abs.diff.rds")
m1_loo <- m1$loo()
m2 <- read_rds("models/task1.alpha.abs.adj.diff.rds")
m2_loo <- m2$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo),
                  method = c("stacking"))
```

The model with scaled variance works better. Let's see in detail what it actually does: 

```{r}
fit <- m2
summary <- fit$summary()

summary %>% filter(str_detect(variable, "^lambda")) %>%
  arrange(median) %>% 
  ggplot(aes(y = reorder(variable, median))) +
  geom_segment(aes(x = q5, xend = q95, yend = variable)) +
  geom_point(aes(x = median)) + xlim(0, 1)
```

```{r}
task1_df %>% mutate(mix_paramter = as_draws_rvars(fit)$lambda) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = mix_paramter)) + 
  ggdist::stat_ribbon() + 
  ylim(0, 1)

task1_df %>% mutate(mix_paramter = as_draws_rvars(fit)$lambda) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = 1 - mix_paramter)) + 
  ggdist::stat_ribbon() + 
  ylim(0, 1)
```

This, however, is still not what we want. When `lambda = 0`, the participant should be selecting mod all the time with little variance, which is not what the above graph is telling us about. 

```{r}
task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = abs(va.med.x - va.mod.x), y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

### lambda = inv-logit(beta * (alpha + abs(x.med - x.mod) / var))

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.6.stan")
model 
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4, 
  adapt_delta = 0.95)
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.beta.abs.adj.diff.alpha.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Checking the posterior results: 

```{r}
#| column: margin 

fit$draws() %>% 
  spread_draws(`alpha`, `beta`) %>%
  pivot_longer(4:5) %>%
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.beta.abs.adj.diff.alpha.rds")
```

### Compare between whether alpha should be intercept outside or inside 

```{r}
m1 <- read_rds("models/task1.alpha.abs.adj.diff.rds")
m1_loo <- m1$loo()
m2 <- read_rds("models/task1.beta.abs.adj.diff.alpha.rds")
m2_loo <- m2$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo),
                  method = c("stacking"))
```

So alpha should be outside instead of inside. 

## Weighted average model 

### The model 

After talking to Matt, Matt said that the process is more similar to anchor then adjust, instead of choosing to anchor or adjust, so the model might be better with weighted average. 

As of now, we don't know what a weighted average^[or convolution integration?] between a Normal distribution and a Laplace distribution is. Let's first start with a Normal + Normal, since we know that the sum is still Normal. 

$$\begin{align*}
    \hat{x}_i &\sim w_i \cdot \hat{x}^\text{med}_i + (1 - w_i) \cdot \hat{x}^\text{mod}_i \\ 
    \hat{x}^\text{med}_i &\sim \mathcal{N}(x^\text{med}_i + \mu^\text{med}, (\sigma^\text{med})^2) \\ 
    \hat{x}^\text{mod}_i &\sim \mathcal{N}(x^\text{mod}_i + \mu^\text{mod}, \sigma^\text{mod}^2) \\ 
    w_i &= \frac{\text{MSE}^{-1}(\hat{x}_i^\text{med})}{\text{MSE}^{-1}(\hat{x}_i^\text{med}) + \text{MSE}^{-1}(\hat{x}_i^\text{mod})}
\end{align*}$$

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.8.stan")
model
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4, 
  # adapt_delta = 0.95
  )
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.weighted.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.weighted.rds")
```

```{r}
as_draws_rvars(fit)$w
```


```{r}
task1_df %>% mutate(weight = as_draws_rvars(fit)$w) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = weight)) + 
  ggdist::stat_ribbon() + ylim(0, 1)

task1_df %>% mutate(weight = as_draws_rvars(fit)$w) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = 1 - weight)) + 
  ggdist::stat_ribbon() + ylim(0, 1)
```

```{r}
# task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
#   unnest_rvars() %>% 
#   ggplot(aes(x = param.lambda, y = y_rep)) + 
#   ggdist::stat_ribbon() + 
#   scale_fill_brewer(palette = "Blues")

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon(alpha = 0.5) +
  geom_point(aes(y = va.select.x - va.med.x), data = task1_df, alpha = 0.8) + 
  scale_fill_brewer(palette = "Blues")

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = abs(va.med.x - va.mod.x), y = y_rep)) + 
  ggdist::stat_lineribbon(alpha = 0.5) +
  geom_point(aes(y = va.select.x - va.med.x), data = task1_df, alpha = 0.8) + 
  scale_fill_brewer(palette = "Blues")
```

```{r}
task1_df %>% ggplot(aes(x = abs(va.med.x - va.mod.x), y = va.select.x - va.med.x)) + geom_point()
  # ggdist::stat_lineribbon(alpha = 0.5) +
  geom_point(aes(y = va.select.x - va.med.x), data = task1_df, alpha = 0.8) + 
  # scale_fill_brewer(palette = "Blues")
```

Matt's suggestion: 

```{r}
custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

### The model with bigger variance 

```{r}
#| eval: false 
model <- cmdstan_model("stan_files/sheng.mix.2.8_big.var.stan")
model
``` 

Fit the model: 

```{r}
#| output: false 
#| eval: false 
fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4, 
  # adapt_delta = 0.95
  )
```

```{r}
#| echo: false
#| output: false 
fit <- readRDS("models/task1.weighted_big.var.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit)
```

Save the stan model: 

```{r}
#| eval: false 

fit$save_object(file = "models/task1.weighted_big.var.rds")
```

```{r}
#| layout-ncol: 2 

task1_df %>% mutate(weight = as_draws_rvars(fit)$w) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = weight)) + 
  ggdist::stat_ribbon() + ylim(0, 1)

task1_df %>% mutate(weight = as_draws_rvars(fit)$w) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = 1 - weight)) + 
  ggdist::stat_ribbon() + ylim(0, 1)
```

```{r}
task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon(alpha = 0.5) +
  geom_point(aes(y = va.select.x - va.med.x), data = task1_df, alpha = 0.8) + 
  scale_fill_brewer(palette = "Blues")

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon(alpha = 0.5) +
  geom_point(aes(y = va.select.x - va.med.x), data = task1_df, alpha = 0.8) + 
  scale_fill_brewer(palette = "Blues")
```

```{r}
custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

## Compare all six models 

```{r}
# TODO 
# m1 <- readRDS(file = "mix_models/sheng.mix.2.1.rds")
# m2 <- readRDS(file = "mix_models/sheng.mix.2.2.1.rds")
# m3 <- readRDS(file = "mix_models/sheng.mix.2.3.rds")
# m4 <- readRDS(file = "mix_models/sheng.mix.2.4.rds")
# m5 <- readRDS(file = "mix_models/sheng.mix.2.5.rds")
# m6 <- readRDS(file = "mix_models/sheng.mix.2.6.rds")
# # calculate loo 
# m1_loo <- m1$loo()
# m2_loo <- m2$loo()
# m3_loo <- m3$loo()
# m4_loo <- m4$loo()
# m5_loo <- m5$loo()
# m6_loo <- m6$loo()
# 
# brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo, "Model 3" = m3_loo, 
#                              "Model 4" = m4_loo, "Model 5" = m5_loo, "Model 6" = m6_loo),
#                   method = c("stacking"))
```
