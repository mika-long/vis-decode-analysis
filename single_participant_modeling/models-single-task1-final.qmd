---
title: "mixture-model"
format: 
  html: 
    reference-location: margin
    toc: true
    code-overflow: wrap
    grid: 
      margin-width: 450px 
---

```{r}
#| echo: false
#| output: false

library(tidyverse)
library(ggplot2)
# setting the theme 
theme_set(theme_minimal())
options(ggplot2.discrete.fill = function() scale_fill_brewer(palette = "Blues"))
# model
library(brms)
# plot results 
library(tidybayes)
library(cmdstanr)
library(sgt)
library(posterior)
```

# Read in data and all processing 

- All the things selected by the participants will be named `xxx.select.xxx` 
- All the things related to the actual answer will be named `xxx.ans.xxx`

```{r}
#| code-fold: true
#| output: false 

df <- read.csv("vis-decode-slider_all_tidy.csv") %>% as_tibble(.)

# filter 
ids <- df %>% count(participantId) %>% filter(n == 492) %>% pull(participantId) 
df <- df %>% filter(participantId %in% ids)

# create a separate dataframe for just test related trials 
task_df <- df %>% filter(grepl("task", trialId) & grepl("test", trialId) ) %>% 
    select(participantId, trialId, responseId, answer) %>% 
    mutate(answer = as.numeric(answer)) %>% 
    pivot_wider(names_from = responseId, values_from = answer, names_repair = "universal") %>% 
  separate_wider_delim(trialId, delim = "_", names = c("task", "type", "id")) %>% 
  select(-type) %>% 
  rename(data.select.x = location.x, 
         data.select.y = location.y,
         pixel.select.x = pixel.x, 
         pixel.select.y = pixel.y)

# create a dataframe for single participant (Sheng only) 
not_sheng <- c("5f37a06e-50e4-4489-948f-c4f25bd38d17", "85349f2b-c75a-46ff-8f80-fafc92da11a7")
single_pid_df <- task_df %>% filter(!participantId %in% not_sheng)
```

```{r}
#| code-fold: true

# origin is top left  
data_to_pixel_y <- function(data_y) {
  return(-395 * data_y + 410)
}
data_to_pixel_x <- function(data_x) {
  return (53.5 * data_x + 317.5)
}

# origin is bottom left 
pixel_to_phy_x <- function(pixel, pxMM){
  (pixel - 50) / pxMM
}
pixel_to_phy_y <- function(pixel, pxMM){
  (410 - pixel) / pxMM
}

# return visual angle in degrees and not radian
vis_angle <- function(size, distance){
  return(2 * atan(size / (2 * distance)) * 180 / pi)
}

# tolerance for numerical precision 
tolerance <- 1e-10
```

```{r}
#| code-fold: true

p <- df %>% filter(participantId %in% ids) %>% 
  filter(grepl("pixelsPerMM", responseId) | grepl("prolificId", responseId)) %>% 
  select(participantId, responseId, answer) %>% 
  pivot_wider(names_from = responseId, values_from = answer) %>% 
  pull(participantId)

# custom dataframe 
pixel_to_mm <- data.frame(participantId = p, 
  pixelToMM = c(3.73, 3.27, 3.27, 5.03, 3.73, 3.25, 3.73, 3.27, 3.73, 3.27, 5.14, 3.30, 3.29)
)

vis_distance <- data.frame(participantId = p, 
                           dist_to_screen = c(426, 502, 500, 495, 485, 987, 635, 500, 479, 563, 449, 685, 462))

# combine 
participants <- pixel_to_mm %>% left_join(vis_distance, by = join_by(participantId))
```

# Task 1 --- split area into equal halves 

Load data: 

```{r}
#| code-fold: true 

task1_df <- single_pid_df %>% filter(task == "task1") %>% # task_df
  select(-slider.x, -slider.y) %>% 
  left_join(participants, by = join_by(participantId)) %>% 
  mutate(data.select.left_area = psgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE), 
         data.ans.x = qsgt(0.5, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE)) %>% 
  # pixel related 
  mutate(pixel.med.x = data_to_pixel_x(data.ans.x), 
         pixel.mod.x = data_to_pixel_x(param.mu)) %>% 
  # phy related 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.med.x = pixel_to_phy_x(pixel.med.x, pixelToMM), 
         phy.mod.x = pixel_to_phy_x(pixel.mod.x, pixelToMM)) %>% 
  # visual angle related 
  mutate(va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.med.x = vis_angle(phy.med.x, dist_to_screen), 
         va.mod.x = vis_angle(phy.mod.x, dist_to_screen))
```

**Note that unless specifically mentioned, all estimations are done in visual angle measures.** 

```{r}
task1_df %>% ggplot(aes(x = param.lambda, y = va.select.x - va.med.x)) + 
  geom_point(alpha = 0.5)
```

# Mixture of Normal and Laplace 

## Weight function is logit 

```{r}

model <- cmdstan_model("stan_files/normal.laplace.logit.stan")
# model$print()
model 
```

Prepare data for stan: 

```{r}

N <- task1_df %>% nrow(.)
x <- task1_df %>% pull(va.select.x)
x_med <- task1_df %>% pull(va.med.x)
x_mod <- task1_df %>% pull(va.mod.x)

stan_data <- list(
  N = N, 
  x = x, 
  x_med = x_med, 
  x_mod = x_mod
)
```

Define custom posterior predictive check function: 

```{r}
#| label: custom pp_check 
custom_pp_check <- function(
   fitted_model, 
   fun = bayesplot::ppc_dens_overlay, 
   ndraws = 100, 
   transform = identity,
   ...
 ) {
   y_rep <- as_draws_rvars(fitted_model)$y_rep |> 
     transform() |>
     as_draws_matrix()
   if (!is.null(ndraws)) {
     y_rep = y_rep |>
       merge_chains() |>
       resample_draws(ndraws = ndraws) 
   }
   y <- fitted_model$draws("x_org", format = "matrix")[1, ] |>
     as.vector() |>
     transform()
   fun(y, y_rep, ...)
 }
```

Fit the stan model: 

```{r}
#| output: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
)

fit$save_object(file = "models/task1.normal.laplace.logit.rds")
```

Posterior predictive checks: 

```{r}
summary <- fit$summary()

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

Checking the posterior results: 

```{r}
#| layout-ncol: 2 
# fit$draws() %>% spread_draws(`theta`) # %>%
#   ggplot(aes(x = theta)) + 
#   stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

```{r}
task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

## Weight function is inv-mse 

```{r}
model <- cmdstan_model("stan_files/normal.laplace.inv_mse.stan")
model
``` 

Fit the stan model: 

```{r}
#| output: false 
#| eval: true 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  # parallel_chains = 4
)

fit$save_object("models/normal.laplace.inv_mse.rds")
```

Posterior predictive checks: 

```{r}
#| column: margin 
summary <- fit$summary()

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

Checking the posterior results: 

```{r}
#| layout-ncol: 2 

# fit$draws() %>% 
#   spread_draws(`theta`) %>%
#   ggplot(aes(x = beta)) + 
#   stat_halfeye()

fit$draws() %>% 
  spread_draws(`mu_med`, `mu_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()

fit$draws() %>% 
  spread_draws(`sigma_med`, `sigma_mod`) %>% 
  pivot_longer(4:5) %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

```{r}
task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

## Normal and Normal 

### Mixture + Logit 

The following has a hard time fitting ... skipping for now ... 

<!-- ```{r} -->
<!-- #| eval: false  -->
<!-- model <- cmdstan_model("stan_files/normal.normal.mix.logit.stan") -->
<!-- model -->
<!-- ```  -->

<!-- ```{r} -->
<!-- #| output: false  -->
<!-- #| eval: false  -->

<!-- fit <- model$sample( -->
<!--   data = stan_data,  -->
<!--   chains = 4,  -->
<!--   adapt_delta = 0.99,  # Increased from default 0.8 -->
<!--   max_treedepth = 15,   # Increased from default 10 -->
<!--   parallel_chains = 4 -->
<!-- ) -->
<!-- ``` -->

<!-- Posterior predictive checks:  -->

<!-- ```{r} -->
<!-- #| column: margin  -->
<!-- summary <- fit$summary() -->

<!-- custom_pp_check(fit) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| layout-ncol: 2  -->

<!-- task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>%  -->
<!--   unnest_rvars() %>%  -->
<!--   ggplot(aes(x = param.lambda, y = y_rep)) +  -->
<!--   ggdist::stat_lineribbon() +  -->
<!--   geom_point(aes(y = va.select.x - va.med.x)) -->

<!-- task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>%  -->
<!--   unnest_rvars() %>%  -->
<!--   ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) +  -->
<!--   ggdist::stat_lineribbon() +  -->
<!--   geom_point(aes(y = va.select.x - va.med.x)) -->
<!-- ``` -->

### Mixture + inv_mse 

```{r}

model <- cmdstan_model("stan_files/normal.normal.mix.inv_mse.stan")
model
``` 

```{r}
#| output: false 


fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  adapt_delta = 0.95 # ,  # Increased from default 0.8
  # max_treedepth = 15,   # Increased from default 10
  # parallel_chains = 4
)

fit$save_object("models/normal.normal.mix.inv_mse.rds")
```

Posterior predictive checks: 

```{r}
#| layout-ncol: 2 

summary <- fit$summary()

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

```{r}
#| layout-ncol: 2 

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

### Weighted Average + Logit 

```{r}

model <- cmdstan_model("stan_files/normal.normal.weight.logit.stan")
model
``` 

```{r}
#| output: false 

fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  adapt_delta = 0.95
)

fit$save_object("models/normal.normal.weight.logit.rds")
```

Posterior predictive checks: 

```{r}
#| layout-ncol: 2 

summary <- fit$summary()

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```

```{r}
#| layout-ncol: 2 

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

### Weighted Average + Inv_MSE

```{r}

model <- cmdstan_model("stan_files/normal.normal.weight.inv_mse.stan")
model
``` 

```{r}
#| output: false 


fit <- model$sample(
  data = stan_data, 
  chains = 4, 
  adapt_delta = 0.95
)

fit$save_object("models/normal.normal.weight.inv_mse.rds")
```

Posterior predictive checks: 

```{r}
#| layout-ncol: 2 
summary <- fit$summary()

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = with(task1_df, case_when(param.lambda < 0 ~ "neg", param.lambda == 0 ~ "zero", param.lambda > 0 ~ "pos")))

custom_pp_check(fit, bayesplot::ppc_dens_overlay_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))

custom_pp_check(fit, bayesplot::ppc_pit_ecdf, transform = \(x) x - task1_df$va.med.x, ndraws = NULL)

custom_pp_check(fit, bayesplot::ppc_pit_ecdf_grouped, group = ifelse(task1_df$param.lambda < 0, "neg", ifelse(task1_df$param.lambda > 0, "pos", "zero")))
```


```{r}
#| layout-ncol: 2 

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = param.lambda, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))

task1_df %>% mutate(y_rep = as_draws_rvars(fit)$y_rep - va.med.x) %>% 
  unnest_rvars() %>% 
  ggplot(aes(x = va.med.x - va.mod.x, y = y_rep)) + 
  ggdist::stat_lineribbon() + 
  geom_point(aes(y = va.select.x - va.med.x))
```

### LOO Compare the above models so far 

```{r}
m1 <- readRDS("models/task1.normal.laplace.logit.rds")
m1_loo <- m1$loo()
m2 <- readRDS("models/normal.laplace.inv_mse.rds") 
m2_loo <- m2$loo()
m3 <- readRDS("models/normal.normal.mix.inv_mse.rds") 
m3_loo <- m3$loo()
m4 <- readRDS("models/normal.normal.weight.inv_mse.rds") 
m4_loo <- m4$loo()
m5 <- readRDS("models/normal.normal.weight.logit.rds") 
m5_loo <- m5$loo()

brms::loo_model_weights(list("Model 1" = m1_loo, "Model 2" = m2_loo, 
                             "Model 3" = m3_loo, "Model 4" = m4_loo, 
                             "Model 5" = m5_loo),
                  method = c("stacking"))
```

The weighted average of normal approach beats the mixture model every time ... 
Which makes sense. We are not picking from one or the other, but more like pick and adjust ... 

