---
title: "Analysis --- Task 4"
---

```{r}
#| echo: false
#| output: false
#| label: setup

library(tidyverse)
library(ggplot2)
library(sgt)
library(numDeriv)
library(ggdist)
# setting the theme 
theme_set(theme_minimal(base_size = 10))
# model
library(brms)
library(rstan)
options(brms.backend = "cmdstanr")
options(mc.cores = parallel::detectCores())
# plot results 
library(tidybayes)
library(here)

# source the helper functions 
source(here("R", "helpers.R"))
```

# Overview / TLDR 

Recall that Task 4 is the task where we ask the participant to find the highest sloped on the CDF (i.e., find mode on CDF). 

We ran the following models: 

- The Exponential model in @sec-exponential 
- The Lomax model in @sec-lomax
- The Weibull model in @sec-weibull --- this is also the best-performing model from LOO model comparison. 
- The Log-Normal model in @sec-logNormal

Finally we compared all models in @sec-model-compare. 

# Set up

Read in data 

```{r}
#| label: read-in-data
#| code-fold: true

task_df <- read_rds(here("data", "final", "task.rds"))
participants <- read_rds(here("data", "final", "participants.rds"))
```

Get task 4 specific data: 

```{r}
#| label: get task 4 data 

data_to_screen_slope <- function(k){
  return(k * 395 / 53.5)
}

task4_df <- task_df %>% filter(task == "task4") %>% 
  select(-slider.x, -slider.y) %>% 
  mutate(data.select.slope = dsgt(data.select.x, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE), 
         data.ans.slope = dsgt(param.mu, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent=FALSE)) %>% 
  right_join(participants, by = join_by(participantId)) %>% 
  mutate(phy.select.x = pixel_to_phy_x(pixel.select.x, pixelToMM), 
         phy.select.y = pixel_to_phy_y(pixel.select.y, pixelToMM),
         va.select.x = vis_angle(phy.select.x, dist_to_screen), 
         va.select.y = vis_angle(phy.select.y, dist_to_screen)) %>%
  mutate(data.ans.x = param.mu, 
         data.ans.y = 1 / 2 * (psgt(data.ans.x + tolerance, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE) +  
                                            psgt(data.ans.x - tolerance, param.mu, param.sigma, param.lambda, param.p, param.q, mean.cent = FALSE))) %>%
  mutate(pixel.ans.x = data_to_pixel_x(data.ans.x), 
         pixel.ans.y = data_to_pixel_y(data.ans.y)) %>% 
  mutate(phy.ans.x = pixel_to_phy_x(pixel.ans.x, pixelToMM), 
         phy.ans.y = pixel_to_phy_y(pixel.ans.y, pixelToMM)) %>% 
  mutate(va.ans.x = vis_angle(phy.ans.x, dist_to_screen), 
         va.ans.y = vis_angle(phy.ans.y, dist_to_screen)) 

```

```{r}
#| code-fold: true 
#| layout-ncol: 2 
#| eval: false 

## Plot relations 

task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) + 
  geom_dots()

task4_df %>% ggplot(aes(x = va.ans.x - va.select.x)) + geom_dots()
task4_df %>% ggplot(aes(x = va.ans.y - va.select.y)) + geom_dots()
task4_df %>% ggplot(aes(x = atan(va.ans.y / va.ans.x) * 180 / pi)) + geom_dots()
task4_df %>% ggplot(aes(x = va.ans.y / va.ans.x, y = data.ans.slope)) + 
  geom_point() + coord_equal()

task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()

task4_df %>% filter(data.select.slope - data.ans.slope == 0)

task4_df %>% ggplot(aes(x = atan(data.select.slope) * 180 / pi - atan(data.ans.slope) * 180 / pi)) + 
  geom_dots()

task4_df %>% ggplot(aes(x = atan(data_to_screen_slope(data.select.slope)) * 180 / pi - atan(data_to_screen_slope(data.ans.slope)) * 180 / pi)) + 
  stat_ecdf()
```

If we map the mathematical slope to the actual slope, this is what we get: 

- y axis range [0,1] correspond to 395 px -> 395px / unit 
- x axis range [-5, 5] correspond to 535 px -> 53.5 px / unit 

```{r}
task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope), 
                        y = data_to_screen_slope(data.ans.slope))) + 
  geom_point(alpha = 0.5) + 
  coord_equal()

task4_df %>% ggplot(aes(x = data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) + 
  geom_dots()
```

Let's first see the distribution of signed error: 

```{r}
task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point(alpha = 0.5)
```

Let's see if there's any obvious relation between signed error and parameters of the distribution: 

```{r}
#| layout-ncol: 2
#| code-fold: true 
#| output: false 

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = dist_to_screen)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.mu)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.sigma)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.lambda)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.p)) + 
  geom_point()

task4_df %>% ggplot(aes(y = data.select.slope - data.ans.slope, x = param.q)) + 
  geom_point()

task4_df %>% ggplot(aes(x = data.select.slope - data.ans.slope)) + 
  geom_dots()
```

# Exponential Model {#sec-exponential}

Math model: 

$$\begin{align}
\text{error_i} &\sim \text{Exponential}(\lambda_{\text{PID}[i]}) \\ 
\log(\lambda_j) &\sim \mathcal{N}(\mu_\lambda, \sigma_\lambda) \\ 
\mu_\lambda &\sim \mathcal{N}(0, 1) \\ 
\sigma_\lambda &\sim \text{Half-Cauchy}(0, 1) 
\end{align}$$

Formula and prior: 

```{r}
f <- bf(error ~ 0,
        mu ~ (1 | participantId),  
        family = exponential())

# mu has log-link, so we don't pose any bounds on it 
p <- c(
  prior(normal(0, 1), class = Intercept), 
  prior(normal(0, 1), class = sd, group = participantId, lb = 0)
)
```

```{r}
#| eval: false 
#| echo: false 

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>% 
  get_prior(f, data = .)

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>% 
  make_stancode(f, data = ., prior = p)
```

Fit the model: 

```{r}
#| output: false 
#| label: fit exponential 

model.exponential <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  # brms need rsp > 0 
  mutate(error = ifelse(abs(error) < tolerance, tolerance, error)) %>% 
  brm(formula = f, 
      data = ., 
      prior = p, 
      chains = 4, 
      cores = 4, 
      file = here("fitted_models", "task4.exponential"),
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE), 
      iter = 6000
  )
```

Check fit: 

```{r}
summary(model.exponential)
plot(model.exponential)
```

```{r}
#| layout-ncol: 2
pp_check(model.exponential, ndraws = 100)
pp_check(model.exponential, ndraws = 100, type = "pit_ecdf")
```

```{r}
#| eval: false 
#| echo: false 

# model.exponential %>% get_variables()

model.exponential %>% spread_draws(b_Intercept, sd_participantId__Intercept) %>% 
  mutate(lambda = exp(b_Intercept))  %>% 
  pivot_longer(5:6)  %>% 
  ggplot(aes(x = value, y = name)) + 
  stat_halfeye()
```

TODO: Overlay plot: 

```{r}
prediction <- task4_df %>% 
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>% 
  mutate(error = ifelse(abs(error) < tolerance, tolerance, error)) %>% 
  add_predicted_draws(model.exponential, re_formula = NA)

prediction %>% ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "skyblue", alpha = 0.8)  + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)))
```

```{r}
task4_df %>% modelr::data_grid(error = 0) %>% 
  add_predicted_draws(model.exponential, ndraws = 100, re_formula = NA) %>% 
  ggplot() + 
  stat_slab(aes(x = .prediction), 
            fill = "lightblue", alpha = 1) + 
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) 
```

# Lomax Model {#sec-lomax}

Lomax is Pareto Type II distribution, which is implemented in `cmdstanr`. We had similar models for task 2. 

```{r}
lomax_family <- custom_family(
  "lomax",
  dpars = c("mu", "lambda", "alpha"),    # includes mu as required
  links = c("identity", "log", "log"),   # appropriate links
  lb = c(-Inf, 0, 0),                    # appropriate bounds
  type = "real"
)

stan_funs <- "
  real lomax_lpdf(real y, real mu, real lambda, real alpha) {
    return pareto_type_2_lpdf(y | 0, lambda, alpha);  # mu is ignored
  }

  real lomax_rng(real mu, real lambda, real alpha) {
    return pareto_type_2_rng(0, lambda, alpha);  # mu is ignored
  }
"
```

Specify formula and prior^[Note that the Half-Cauchy prior for hyperpriors led to divergent transitions, so we switched to truncated normal for priors]: 

```{r}
# Define the formula for the model
f <- bf(
  error ~ 0,
  mu ~ 0,
  alpha ~ (1 | participantId),
  lambda ~ (1 | participantId)
)

p <- c(
    prior(normal(0, 1), class = Intercept, dpar = alpha),
    prior(normal(0, 1), class = sd, group = participantId, dpar = alpha, lb = 0),
    prior(normal(0, 1), class = Intercept, dpar = lambda),
    prior(normal(0, 1), class = sd, group = participantId, dpar = lambda, lb = 0)
  )
```

```{r}
#| eval: false
#| echo: false

task4_df %>%  mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>%
  get_prior(formula = f, family = lomax_family)

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope)) + tolerance) %>%
  make_stancode(formula = f, family = lomax_family, prior = p)
```

```{r}
#| output: false
#| label: fit lomax

model.lomax <- task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>%
  brm(
    formula = f,
    data = .,
    family = lomax_family,
    prior = p,
    stanvars = stanvar(scode = stan_funs, block = "functions"),
    chains = 4,
    cores = 4,
    file = here("fitted_models", "task4.lomax"),
    file_refit = "on_change",
    control = list(adapt_delta = 0.99),
    iter = 4000,
    save_pars = save_pars(all = TRUE)
  )
```

Check fit:

```{r}
# pairs(model.lomax)
summary(model.lomax)
plot(model.lomax)
```

```{r}
#| eval: false 
#| echo: false 

# Check the posterior distribution of parameters, alpha and lambda:

model.lomax %>% spread_draws(`^b_.*`, regex = TRUE) %>%
  mutate(b_alpha = exp(b_alpha_Intercept), 
         b_lambda = exp(b_lambda_Intercept)) %>%
  pivot_longer(6:7) %>%
  ggplot(aes(x = value, y = name)) +
  stat_halfeye() +
  facet_wrap(~name, scales = "free") +
  geom_vline(xintercept = 1e-10, linetype = "dashed", color = "gray")
```

Define custom functions^[Check out this [link](https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html?utm_source=chatgpt.com) for more details.] for posterior predictive check (`pp_check`) and model comparison (`loo`):

```{r}
expose_functions(model.lomax, vectorize = TRUE)

# Define the posterior_predict function in R as per vignette
posterior_predict_lomax <- function(i, draws, ...) {
  alpha <- draws$dpars$alpha[i]
  lambda <- draws$dpars$lambda[i]
  # Use Stan's rng function indirectly via brms
  lomax_rng(0, lambda, alpha)
}

log_lik_lomax <- function(i, prep){
  alpha <- brms::get_dpar(prep, "alpha", i = i)
  lambda <- brms::get_dpar(prep, "lambda", i = i)
  y <- prep$data$Y[i]
  lomax_lpdf(y, 0, lambda, alpha)
}
```

```{r}
#| layout-ncol: 2
pp_check(model.lomax, ndraws = 100)
pp_check(model.lomax, ndraws = 100, type = "pit_ecdf")
```

Overlay plot:

```{r}
prediction <- task4_df %>%
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
  add_predicted_draws(model.lomax, ndraws = 100)

prediction %>% ggplot() +
  stat_slab(aes(x = .prediction),
            fill = "lightblue", alpha = 1) +
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)))
```

# Weibull distribution {#sec-weibull}

- [wikipedia](https://en.wikipedia.org/wiki/Weibull_distribution)
- [stan](https://mc-stan.org/docs/functions-reference/positive_continuous_distributions.html#weibull-distribution)
- it's also implemented in brms! So unlike the [Lomax model](@sec-lomax), we don't need to write custom functions. 

Specify formula and prior: 

```{r}
f <- bf(
  error ~ 0,
  mu ~ (1 | participantId), # log link
  shape ~ (1 | participantId), # log link
  family = weibull
)

p <- c(
    prior(normal(0, 1), class = Intercept),
    prior(normal(0, 1), class = sd, group = participantId, lb = 0),
    prior(normal(0, 1), class = Intercept, dpar = shape),
    prior(normal(0, 1), class = sd, group = participantId, dpar = shape, lb = 0)
  )
```

```{r}
#| eval: false
#| echo: false

task4_df %>%  mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>%
  get_prior(formula = f)

task4_df %>% mutate(error = abs(data_to_screen_slope(data.select.slope) - data_to_screen_slope(data.ans.slope)) + tolerance) %>%
  make_stancode(formula = f, prior = p)
```

Fit the model:

```{r}
#| output: false
#| label: fit weibull

model.weibull <- task4_df %>%
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% # brms needs value to be > 0
  brm(formula = f,
      data = .,
      prior = p,
      chains = 4,
      cores = 4,
      file = here("fitted_models", "task4.weibull"), 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE),
      iter = 6000
  )
```

Check fit and posterior:

```{r}
summary(model.weibull)
plot(model.weibull)
```

```{r}
#| layout-ncol: 2
pp_check(model.weibull, ndraws = 100) +
  coord_cartesian(xlim = c(0, 10)) +
  theme(text = element_text(size = 10)) -> p1

pp_check(model.weibull, ndraws = 100, type = "pit_ecdf") +
  theme(text = element_text(size = 10)) -> p2

p1
p2
```

```{r}
#| echo: false 

ggsave(here("figs", "task4-pp-check.pdf"), p1, 
       height = 5, width = 10, units = "cm", dpi = 300)

ggsave(here("figs", "task4-pit-ecdf.pdf"), p2, 
       height = 5, width = 10, units = "cm", dpi = 300)
```

```{r}
#| eval: false 
#| echo: false 

# model.weibull %>% get_variables()

model.weibull %>% spread_draws(b_Intercept, b_shape_Intercept) %>%
  mutate(mu = exp(b_Intercept), shape = exp(b_shape_Intercept)) %>%
  pivot_longer(6:7) %>%
  ggplot(aes(x = value, y = name)) +
  stat_halfeye()
```

Overlay plot:

```{r}
#| code-fold: true 
#| layout-ncol: 2

# prediction <- task4_df %>%
#   mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
#   add_predicted_draws(model.weibull, ndraws = 100)

task4_df %>% modelr::data_grid(error = 0) %>%
  add_predicted_draws(model.weibull, re_formula = NA) %>%
  ggplot() +
  stat_slab(aes(x = .prediction),
            fill = "lightblue", alpha = 1) +
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)))

task4_df %>% modelr::data_grid(error = 0) %>%
  add_predicted_draws(model.weibull, re_formula = NA) %>%
  ggplot() +
  stat_slab(aes(x = .prediction),
            fill = "lightblue", alpha = 1) +
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) +
  labs(y = "Probability", x = "Signed error") +
  coord_cartesian(xlim=c(0, 5))
```

Posterior draws + strip plot of actual answers: 

```{r}
#| code-fold: true
#| layout-ncol: 3

task4_df %>% modelr::data_grid(error = 0,
                               participantId = unique(participantId)) %>%
  add_predicted_draws(model.weibull, ndraws = 100) %>%
  ggplot() +
  stat_slab(aes(x = .prediction, group = .draw),
               color = "lightblue", alpha = 0.5, fill = NA, 
            n = 3000, linewidth = 0.5) +
  scale_thickness_shared() +
  coord_cartesian(xlim = c(0, 4)) +
  geom_rug(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)), size = 0.5) +
  stat_slab(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)),
            color = "darkblue", fill = NA, n = 3000, size = 0.5) +
  labs(y = "density", x = "signed error" ) +
  scale_y_continuous(breaks = 0) -> p

p
```

```{r}
#| echo: false 
ggsave(here("figs", "task4.pdf"), plot = p, 
       height = 6, width = 8, units = "cm", dpi = 300)
```

# Log-Normal distribution {#sec-logNormal}

Formula and prior: 

```{r}
f <- bf(error ~ 0,
        mu ~ (1 | participantId),
        sigma ~ (1 | participantId),
        family = lognormal())

p <- c(
  prior(normal(0, 1), class = Intercept),
  prior(normal(0, 1), class = sd, group = participantId, lb = 0),
  prior(normal(0, 1), class = Intercept, dpar = sigma),
  prior(normal(0, 1), class = sd, group = participantId, dpar = sigma, lb = 0)
)
```

```{r}
#| echo: false
#| eval: false

task4_df %>%
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
  get_prior(data = ., formula = f)
```

Fit the model: 

```{r}
#| output: false
#| label: fit lognormal

model.lognormal <- task4_df %>%
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
  mutate(error = ifelse(error < tolerance, tolerance, error)) %>% # brms needs value to be > 0
  brm(formula = f,
      data = .,
      prior = p,
      chains = 4,
      cores = 4,
      file = here("fitted_models", "task4.lognormal"), 
      file_refit = "on_change",
      save_pars = save_pars(all = TRUE),
      control = list(adapt_delta = 0.99),
      iter = 4000
  )
```

Check fit and posterior:

```{r}
summary(model.lognormal)
plot(model.lognormal)
```

```{r}
#| layout-ncol: 2
pp_check(model.lognormal, ndraws = 100)
pp_check(model.lognormal, ndraws = 100, type = "pit_ecdf")
```

Overlay plot:

```{r}
prediction <- task4_df %>%
  mutate(error = abs(data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) %>%
  add_predicted_draws(model.lognormal, ndraws = 100)

prediction %>% ggplot() +
  stat_slab(aes(x = .prediction),
            fill = "lightblue", alpha = 1) +
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope))) +
  xlim(0, 10)
```

```{r}
task4_df %>% modelr::data_grid(error = 0) %>%
  add_predicted_draws(model.lognormal, re_formula = NA, ndraws = 100) %>%
  ggplot() +
  stat_slab(aes(x = .prediction),
            fill = "lightblue", alpha = 1) +
  geom_dots(data = task4_df, aes(x = data_to_screen_slope(data.ans.slope) - data_to_screen_slope(data.select.slope)))
```

# Model Comparison {#sec-model-compare}

```{r}
loo(model.exponential, model.lomax, model.weibull, model.lognormal, 
    moment_match = TRUE) 
```

The Weibull model performs the best. 

```{r}
#| layout-ncol: 2

pp_check(model.exponential, ndraws = 100, type = "pit_ecdf") + 
  labs(title = "Exponential")
pp_check(model.lomax, ndraws = 100, type = "pit_ecdf") + 
  labs(title = "Lomax")
pp_check(model.weibull, ndraws = 100, type = "pit_ecdf") + 
  labs(title = "Weibull")
pp_check(model.lognormal, ndraws = 100, type = "pit_ecdf") + 
  labs(title = "Log-Normal")
```

`pit_ecdf` is the empirical CDF of PIT (Probability Integral Transform) values. This helps us assess whether the model is well-calibrated; if it is, then the PIT ECDF plot should follow the 45 degree diagonal line, which is what we see for Weibull. 

